{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"src\"))\n",
    "import glob\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gopro QAQC round 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe of all images and associated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbs_label_path = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\GoPro_datasets\\LMBS\\labels\"\n",
    "lmbs_label_files = Utils.list_files(lmbs_label_path, \".txt\")\n",
    "\n",
    "lmbs_image_path = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\GoPro_datasets\\LMBS\\images\"\n",
    "lmbs_image_files = Utils.list_files(lmbs_image_path, \".jpg\")\n",
    "assert len(lmbs_label_files) == len(lmbs_image_files), \"Label and image file counts do not match.\"\n",
    "\n",
    "glsc_label_path = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\GoPro_datasets\\GLSC\\GLSC_unique_qaqc\\labels\"\n",
    "glsc_label_files = Utils.list_files(glsc_label_path, \".txt\")\n",
    "\n",
    "glsc_image_path = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\GoPro_datasets\\GLSC\\GLSC_unique_qaqc\\images\"\n",
    "glsc_image_files = Utils.list_files(glsc_image_path, \".jpg\")\n",
    "assert len(glsc_label_files) == len(glsc_image_files), \"Label and image file counts do not match.\"\n",
    "'''\n",
    "1075\n",
    "2173'''\n",
    "print(len(lmbs_image_files))\n",
    "print(len(glsc_image_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Update month_map to handle both 'sep' and 'sept'\n",
    "month_map = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'aug': 8, 'sep': 9, 'sept': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "\n",
    "# Create dataframe from LMBS image file paths\n",
    "lmbs_df = pd.DataFrame({'image_path': lmbs_image_files})\n",
    "\n",
    "# Extract the date string (e.g., \"01aug23\" or \"01sept23\") from the filename\n",
    "def extract_date_from_path(path):\n",
    "    match = re.search(r'(\\d{2}[a-z]{3,4}\\d{2})', os.path.basename(path), re.IGNORECASE)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "lmbs_df['date_str'] = lmbs_df['image_path'].apply(extract_date_from_path)\n",
    "\n",
    "# Parse day, month, year from the date string\n",
    "def parse_date(date_str):\n",
    "    if date_str:\n",
    "        day = int(date_str[:2])\n",
    "        # month can be 3 or 4 characters\n",
    "        if date_str[2:6].lower() == 'sept':\n",
    "            month = month_map['sept']\n",
    "            year = int('20' + date_str[6:])\n",
    "        else:\n",
    "            month = month_map[date_str[2:5].lower()]\n",
    "            year = int('20' + date_str[5:])\n",
    "        return day, month, year\n",
    "    return None, None, None\n",
    "\n",
    "lmbs_df[['day', 'month', 'year']] = lmbs_df['date_str'].apply(lambda x: pd.Series(parse_date(x)))\n",
    "\n",
    "lmbs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbs_df[(lmbs_df.year==2022)&(lmbs_df.month==5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## round 2 checks\n",
    "\n",
    "# Combine all image file paths from LMBS and GLSC datasets\n",
    "imgs_all = lmbs_image_files + glsc_image_files\n",
    "\n",
    "# Combine all label file paths from LMBS and GLSC datasets\n",
    "lbls_all = lmbs_label_files + glsc_label_files\n",
    "\n",
    "# Extract unique IDs from image file names (without extensions)\n",
    "imgs_all_ids = list(map(lambda x: os.path.basename(x).split(\".\")[0], imgs_all))\n",
    "\n",
    "# Extract unique IDs from label file names (without extensions)\n",
    "lbls_all_ids = list(map(lambda x: os.path.basename(x).split(\".\")[0], lbls_all))\n",
    "\n",
    "# Ensure there are no duplicate IDs in the label files\n",
    "assert len(lbls_all_ids) == len(list(set(lbls_all_ids)))\n",
    "\n",
    "# Ensure there are no duplicate IDs in the image files\n",
    "assert len(imgs_all_ids) == len(list(set(imgs_all_ids)))\n",
    "\n",
    "# Ensure that the IDs from labels match the IDs from images\n",
    "assert lbls_all_ids == imgs_all_ids\n",
    "\n",
    "# Print the total number of image files\n",
    "print(len(imgs_all)) # 3248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return a dataframe of all images and labels to performa a train/test/validation split\n",
    "# gopro_all = generate_splits(img_pth_lst = imgs_all, bbox_pths=lbls_all, mer_pths=None).return_merged()\n",
    "# functions to check the image size distributions to determine the best size to train YOLO\n",
    "def get_im_w(image_path):\n",
    "    im = Image.open(image_path)\n",
    "    w, h = im.size\n",
    "    return w\n",
    "def get_im_h(image_path):\n",
    "    im = Image.open(image_path)\n",
    "    w, h = im.size\n",
    "    return h\n",
    "lim_w = lambda f: get_im_w(f)\n",
    "lim_h = lambda f: get_im_h(f)\n",
    "# gopro_all[\"imh\"], gopro_all[\"imw\"] = gopro_all.image_path.apply(lim_h), gopro_all.image_path.apply(lim_w)\n",
    "gopro_all = pd.read_csv(r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\GoPro_datasets\\COMBINED\\GLSC_LMBS_dataset - round2\\all_images_label_paths.csv\")\n",
    "gopro_all\n",
    "# gopro_all.groupby(by=[\"imh\",\"imw\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        | imh  | imw  | image_path | Filename | image_id | bbox_path | mer_path | year |\n",
    "        |------|------|------------|----------|----------|-----------|----------|------|\n",
    "        | 1080 | 1920 | 40         | 40       | 40       | 40        | 0        | 0    |\n",
    "        | 1440 | 1920 | 67         | 67       | 67       | 67        | 0        | 0    |\n",
    "        | 2304 | 3072 | 83         | 83       | 83       | 83        | 0        | 0    |\n",
    "        | 2760 | 3680 | 277        | 277      | 277      | 277       | 0        | 0    |\n",
    "        | 2880 | 3840 | 14         | 14       | 14       | 14        | 0        | 0    |\n",
    "        | 3000 | 4000 | 2033       | 2033     | 2033     | 2033      | 0        | 0    |\n",
    "        | 3888 | 5184 | 734        | 734      | 734      | 734       | 0        | 0    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do a train/test/valid split on the dataframe\n",
    "def do_train_test_valid_split(df, train_split=0.7, valid_split=0.2):\n",
    "    l = len(df)\n",
    "    print(\"n samples:\", l)\n",
    "    train_df = df.sample(int(train_split*l), random_state=42)\n",
    "    df = df.drop(train_df.index)\n",
    "    valid_df = df.sample(int(valid_split*l), random_state=42)\n",
    "    df = df.drop(valid_df.index)\n",
    "    test_df = df\n",
    "    X_train, y_train = train_df.image_path.values, train_df.bbox_path.values\n",
    "    X_valid, y_valid = valid_df.image_path.values, valid_df.bbox_path.values\n",
    "    X_test, y_test = test_df.image_path.values, test_df.bbox_path.values\n",
    "    print(f\"training, testing, validation, {X_train.shape[0]}, {X_test.shape[0]},{X_valid.shape[0]}\")\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, train_df, valid_df, test_df\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test, train_df, valid_df, test_df = do_train_test_valid_split(gopro_all, train_split=0.8, valid_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy images and labels\n",
    "import shutil\n",
    "def cpy_lbls(set, dst):\n",
    "    if not os.path.exists(dst): os.makedirs(dst)\n",
    "    for item in set:\n",
    "        shutil.copy2(item, dst)\n",
    "root =  r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Proj_GobyFinder\\gobyfinder_yolov8\\datasets\\GoPro_datasets\\COMBINED\\GLSC_LMBS_dataset - round2\"\n",
    "# cpy_lbls(y_train, os.path.join(root, \"train\\\\labels\"))\n",
    "# cpy_lbls(y_valid, os.path.join(root, \"validation\\\\labels\"))\n",
    "# cpy_lbls(y_test, os.path.join(root, \"test\\\\labels\"))\n",
    "# cpy_lbls(X_train, os.path.join(root, \"train\\\\images\"))\n",
    "# cpy_lbls(X_valid, os.path.join(root, \"validation\\\\images\"))\n",
    "# cpy_lbls(X_test, os.path.join(root, \"test\\\\images\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches for MakeSense.ai QAQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\"datasets\\GoPro_datasets\\GLSC_unique_qaqc\"\n",
    "def create_image_label_batches(root, batch_size=100):\n",
    "    batch_folder = \"batches\"\n",
    "    images = sorted(glob.glob(os.path.join(root, \"images\", \"*.jpg\")))\n",
    "    labels = sorted(glob.glob(os.path.join(root, \"labels\", \"*.txt\")))\n",
    "    assert len(images) == len(labels), \"Number of images and labels do not match\"\n",
    "\n",
    "    # Helper function to split a list into chunks\n",
    "    def chunk_list(lst, chunk_size):\n",
    "        return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "    # Create chunks of images and labels\n",
    "    image_chunks = chunk_list(images, batch_size)\n",
    "    label_chunks = chunk_list(labels, batch_size)\n",
    "\n",
    "    for idx, (image_chunk, label_chunk) in enumerate(zip(image_chunks, label_chunks)):\n",
    "        batch_dir = os.path.join(root, batch_folder, f\"batch_{idx}\")\n",
    "        image_dst = os.path.join(batch_dir, \"images\")\n",
    "        label_dst = os.path.join(batch_dir, \"labels\")\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(image_dst, exist_ok=True)\n",
    "        os.makedirs(label_dst, exist_ok=True)\n",
    "\n",
    "        # Copy images and labels to their respective batch directories\n",
    "        for img_src, lbl_src in zip(image_chunk, label_chunk):\n",
    "            shutil.copy2(img_src, image_dst)\n",
    "            shutil.copy2(lbl_src, label_dst)\n",
    "\n",
    "        # Create a labels.txt file listing all label files in the batch\n",
    "        labels_list_path = os.path.join(batch_dir, \"labels.txt\")\n",
    "        label_files = [os.path.basename(lbl) for lbl in label_chunk]\n",
    "        with open(labels_list_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(label_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMBS Cage Goby Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to combine the the labels and cage boxes for LMBS images\n",
    "def process_labels(label_list):\n",
    "    labels_df = pd.DataFrame(columns=[\"Filename\", \"cls\", \"x\", \"y\", \"w\", \"h\"])\n",
    "    for label in label_list:\n",
    "        Filename = os.path.basename(label).split(\".\")[0]\n",
    "        # read the label file\n",
    "        with open(label, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        # check if the file is empty\n",
    "        if len(lines) == 0:\n",
    "            labels_df = pd.concat([labels_df, pd.DataFrame([{\"Filename\": Filename, \"cls\": np.nan, \"x\": np.nan, \"y\": np.nan, \"w\": np.nan, \"h\": np.nan}])], ignore_index=True)\n",
    "        # extract the name and coordinates\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                name, x, y, w, h = parts\n",
    "            # append to the dataframe\n",
    "            labels_df = pd.concat([labels_df, pd.DataFrame([{\"Filename\": Filename, \"cls\": name, \"x\": float(x), \"y\": float(y), \"w\": float(w), \"h\": float(h)}])], ignore_index=True)\n",
    "    return labels_df\n",
    "\n",
    "def intersection_df(fish_box_df, cage_box_df, img_label_filepath_df, input_type=\"bboxes\"): # 'label' or 'dectect'\n",
    "    # Merge the labels dataframe with the cage bounding boxes dataframe on the \"Filename\" column\n",
    "    # Use a left join and add suffixes to distinguish columns from the two dataframes\n",
    "    df_all = pd.merge(fish_box_df, cage_box_df, on=\"Filename\", how=\"left\", suffixes=(\"_f\", \"_b\"))\n",
    "\n",
    "    # Ensure the number of rows in the merged dataframe matches the number of rows in the labels dataframe\n",
    "    assert len(df_all) == len(fish_box_df), \"lengths do not match\"\n",
    "\n",
    "\n",
    "    if input_type == \"labels\":\n",
    "        # Merge the resulting dataframe with the gopro_all dataframe to include image height and width\n",
    "        df_all = pd.merge(df_all, img_label_filepath_df, on=\"Filename\", how=\"left\")\n",
    "        # create a unique fish ID for each fish in the image\n",
    "        fish_id = df_all.groupby(\"Filename\").cumcount()\n",
    "        fish_id = df_all.groupby(\"Filename\").cumcount()\n",
    "        df_all[f\"{input_type}_id\"] = df_all[\"Filename\"] + \"_\" + fish_id.astype(str)\n",
    "        df_all[\"conf\"] = 1.0\n",
    "\n",
    "    # Define the calculate_intersection class if not already defined\n",
    "    df_all['intersection'] = df_all.apply(\n",
    "        lambda row: calculate_intersection().get_intersection(row) if not row.isnull().any() else np.nan,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df_all[\"inside\"] = np.where(df_all.intersection>0.5, 1, 0)\n",
    "    df_all = df_all[['Filename', 'cls_f', 'x_f', 'y_f', 'w_f', 'h_f', 'x_b', 'y_b', 'w_b', 'h_b', 'imh', 'imw', f'{input_type}_id', 'intersection', 'inside', 'conf']]\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annotated Labels analysis \n",
    "test_set = [os.path.basename(x).split(\".\")[0] for x in X_test]\n",
    "# Define the root directory for LMBS unique QAQC round 2 labels\n",
    "root = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\GoPro_datasets\\LMBS\"\n",
    "\n",
    "# Get a sorted list of all label files in the directory\n",
    "bbox_list = sorted(glob.glob(os.path.join(root, \"labels\", \"*.txt\")))\n",
    "\n",
    "# Get a sorted list of all cage bounding box files in the directory\n",
    "cage_box_list = sorted(glob.glob(os.path.join(root, \"cages\", \"all_boxes\", \"*.txt\")))\n",
    "\n",
    "# dataframe of all images and labels with their file paths\n",
    "img_label_filepath_df = gopro_all[[\"Filename\", \"imh\", \"imw\"]]\n",
    "\n",
    "# Process the label files into a dataframe\n",
    "label_box_df = process_labels(bbox_list)\n",
    "\n",
    "# Process the cage bounding box files into a dataframe\n",
    "cage_box_df = process_labels(cage_box_list)\n",
    "\n",
    "# Perform the intersection analysis for the annotated labels, and save the results to a CSV file\n",
    "df_gopro_label_analysis = intersection_df(label_box_df, cage_box_df, img_label_filepath_df, input_type=input_type)\n",
    "df_gopro_label_analysis[\"test_set\"] = np.where(df_gopro_label_analysis[\"Filename\"].isin(test_set), 1, 0)\n",
    "\n",
    "input_type=\"labels\"\n",
    "# df_gopro_label_analysis.to_csv(os.path.join(root, f\"{input_type}_with_cages_intersection.csv\"), index=False)\n",
    "df_gopro_label_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GobyFinder Gopro prediction analysis \n",
    "test_set = [os.path.basename(x).split(\".\")[0] for x in X_test]\n",
    "\n",
    "# Get a sorted list of all cage bounding box files in the directory\n",
    "cage_box_list = sorted(glob.glob(os.path.join(root, \"cages\", \"all_boxes\", \"*.txt\")))\n",
    "\n",
    "# dataframe of all images and labels with their file paths\n",
    "img_label_filepath_df = gopro_all[[\"Filename\", \"imh\", \"imw\"]]\n",
    "\n",
    "# Process the label files into a dataframe\n",
    "prediction_box_df = pd.read_csv(r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Proj_GobyFinder\\gobyfinder_yolov8\\inference\\GoPro_Round2_Inference\\GoPro_Round2_Inference_predictions.csv\", index_col=0)\n",
    "\n",
    "# set the min confidence level to filter the predictions\n",
    "prediction_box_df = prediction_box_df[prediction_box_df.conf>=0.1]\n",
    "\n",
    "# Process the cage bounding box files into a dataframe\n",
    "cage_box_df = process_labels(cage_box_list)\n",
    "\n",
    "\n",
    "# Perform the intersection analysis for the annotated labels, and save the results to a CSV file\n",
    "input_type=\"detect\"\n",
    "df_gopro_prediction_analysis = intersection_df(prediction_box_df, cage_box_df, img_label_filepath_df, input_type=input_type)\n",
    "df_gopro_prediction_analysis[\"test_set\"] = np.where(df_gopro_prediction_analysis[\"Filename\"].isin(test_set), 1, 0)\n",
    "df_gopro_prediction_analysis.to_csv(os.path.join(root, f\"{input_type}_with_cages_intersection.csv\"), index=False)\n",
    "df_gopro_prediction_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start here to calculate LBMS numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram for the intersection column\n",
    "plt.figure(figsize=(4, 3))\n",
    "df_all = df_gopro_label_analysis\n",
    "df_all[(df_all.intersection > 0) & (df_all.intersection < 1)].intersection.hist(bins=30, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of Intersection Values', fontsize=14)\n",
    "plt.xlabel('Intersection', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Customize ticks\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Save the plot as a high-resolution image (optional)\n",
    "# plt.savefig('intersection_histogram.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random as rand\n",
    "\n",
    "images = glob.glob(r\"C:\\Users\\ageglio\\ageglio-1\\gobyfinder_yolov8\\Gopro_plot_imgs\\imgs\\*\")\n",
    "seed=1984\n",
    "rand.seed(seed)  # Set the random seed for reproducibility\n",
    "images = rand.sample(images, 16)\n",
    "\n",
    "fix, ax = plt.subplots(4, 4, figsize=(9, 7))\n",
    "for i, img_path in enumerate(images):  # Limit to the first 16 images\n",
    "    img = Image.open(img_path)\n",
    "    w = int(img.size[0] / 2)\n",
    "    h = int(img.size[1] / 2)\n",
    "    hor = rand.randint(0, img.size[0] - w)\n",
    "    vert = rand.randint(0, img.size[1] - h)\n",
    "    # Take a random zoomed sample of the image\n",
    "    img = img.crop((hor, vert, hor + w, vert + h))\n",
    "    img = img.resize((840, 640))\n",
    "    ax[i // 4, i % 4].imshow(img)\n",
    "    ax[i // 4, i % 4].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure with 600 dpi\n",
    "plt.savefig(f\"Gopro_plot_imgs\\\\sampled_images{seed}.jpg\", dpi=600, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
