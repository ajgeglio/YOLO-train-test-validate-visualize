{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from src import *\n",
    "import src\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_labels(img_pth_lst=img_pth_lst, lbl_msk_pths = lbl_msk_pths).lbl_masks_to_img_lbls(save_path = r\"Z:\\__Organized_Directories_InProgress\\Annotated_Images\\GoPro_FishSegmentation\\GoPro_FishSegmentation_FullSize\\Dan_Dataset\\YOLO_labels\", color=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_im_w(image_path):\n",
    "    im = Image.open(image_path)\n",
    "    w, h = im.size\n",
    "    return w\n",
    "def get_im_h(image_path):\n",
    "    im = Image.open(image_path)\n",
    "    w, h = im.size\n",
    "    return h\n",
    "lim_w = lambda f: get_im_w(f)\n",
    "lim_h = lambda f: get_im_h(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_test_valid_split(df, train_split=0.6, valid_split=0.2):\n",
    "    l = len(df)\n",
    "    print(\"n samples:\", l)\n",
    "    train_df = df.sample(int(train_split*l), random_state=42)\n",
    "    df = df.drop(train_df.index)\n",
    "    valid_df = df.sample(int(valid_split*l), random_state=42)\n",
    "    df = df.drop(valid_df.index)\n",
    "    test_df = df\n",
    "    X_train, y_train = train_df.image_path.values, train_df.lbl_path.values\n",
    "    X_valid, y_valid = valid_df.image_path.values, valid_df.lbl_path.values\n",
    "    X_test, y_test = test_df.image_path.values, test_df.lbl_path.values\n",
    "    print(f\"training, testing, validation, {X_train.shape[0]}, {X_test.shape[0]},{X_valid.shape[0]}\")\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test, train_df, valid_df, test_df = do_train_test_valid_split(imgs_lbls_gopro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to save the img/lbl in the same text file\n",
    "def save_list(x, y, name):\n",
    "    with open(f\"{name}.txt\", \"w\") as f:\n",
    "        for X, Y in zip(x,y):\n",
    "            f.write('%s\\n' %X)\n",
    "            f.write('%s\\n' %Y)\n",
    "# save_list(X_train, y_train, \"train\")\n",
    "# save_list(X_test, y_test, \"test\")\n",
    "# save_list(X_valid, y_valid, \"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count objects in set\n",
    "def count_objects(which_set):\n",
    "    folder = f\"datasets\\\\datasets_gopro\\\\{which_set}\\\\labels\\\\*txt\"\n",
    "    label_list = glob.glob(folder)\n",
    "    i = 0\n",
    "    n = 0\n",
    "    for label_file in label_list:\n",
    "        try:\n",
    "            df = pd.read_csv(label_file, delimiter=' ', header=None)\n",
    "            num_objects = df.shape[0]\n",
    "            i += num_objects\n",
    "        except:\n",
    "            n += 1\n",
    "    print(f\"num objects in {which_set} is {i}, num images no fish is {n}\")\n",
    "count_objects('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy images and labels\n",
    "import shutil\n",
    "def cpy_lbls(set, dst):\n",
    "    for item in set:\n",
    "        shutil.copy(item, dst)\n",
    "# cpy_lbls(y_train, r\"datasets\\datasets_gopro\\train\\labels\")\n",
    "# cpy_lbls(y_valid, r\"datasets\\datasets_gopro\\validation\\labels\")\n",
    "# cpy_lbls(y_test, r\"datasets\\datasets_gopro\\test\\labels\")\n",
    "# cpy_lbls(X_train, r\"datasets\\datasets_gopro\\train\\images\")\n",
    "# cpy_lbls(X_valid, r\"datasets\\datasets_gopro\\validation\\images\")\n",
    "# cpy_lbls(X_test, r\"datasets\\datasets_gopro\\test\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot label overlay\n",
    "# i = 0\n",
    "import PIL\n",
    "from PIL import ImageDraw\n",
    "i=22 #<---Lots of goby, 6964\n",
    "\n",
    "print(i)\n",
    "img_file = X_train[i]\n",
    "print(img_file)\n",
    "def display_lbl(X_train, i):\n",
    "    img_file = X_train[i]\n",
    "    label = y_train[i]\n",
    "    df = pd.read_csv(label, delimiter=\" \", header=None)\n",
    "    print(df)\n",
    "    imgp = PIL.Image.open(img_file)\n",
    "    im_array = np.array(imgp)\n",
    "    im_h, im_w = im_array.shape[0], im_array.shape[1]\n",
    "    draw = PIL.ImageDraw.Draw(imgp)\n",
    "    s=5\n",
    "    for index, row in df.iterrows():\n",
    "        x, y, w, h = row[1]*im_w, row[2]*im_h, row[3]*im_w, row[4]*im_h\n",
    "        x1 = x - w/2\n",
    "        y1 = y - h/2\n",
    "        x2 = x + w/2\n",
    "        y2 = y + h/2\n",
    "        draw.rectangle((x1,y1,x2,y2), outline=(0, 0, 0), width=5)\n",
    "        draw.ellipse((x-s,y-s,x+s,y+s), fill='red')\n",
    "    return imgp\n",
    "display_lbl(X_train, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New scores after manual Annotation\n",
    "1. Run 02_YOLO_predict.py with the appropriate weights and images with corresponding labels\n",
    "2. Generate a scores sheet with 04_YOLO_calc_scores_df.ipynb\n",
    "3. Generate annotated images with that scores sheet\n",
    "4. Create the base annotation template then add the formulas for eas of use\n",
    "5. After annotation is complete, create the updated scores sheet to compare metrics\n",
    "6. Calculate the change in metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Generate annotated images\n",
    "new_path = r\"Z:\\Proj_Yolo\\GobyFinder_GoPro_Model_Results\\Train_edit\\lbl_imgs\"\n",
    "img_pths = glob.glob(r\"C:\\Users\\ageglio\\ageglio-1\\gobyfinder_yolov8\\datasets\\datasets_gopro\\train\\images\\*.jpg\")\n",
    "scores_df = pd.read_csv(r\"Z:\\Proj_Yolo\\GobyFinder_GoPro_Model_Results\\Train_edit\\yolov8x_GoPro_train_test_scores.csv\", index_col=0)\n",
    "# for img_pth in img_pths:\n",
    "#     src.generate_labels().save_annot_imgs(img_pth, scores_df, new_path, background=\"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Create the base annotation template then add the formulas for eas of use\n",
    "\n",
    "## Manually added formulas into the resulting excel sheet for better user experience\n",
    "## CELL G --> =IF(F2=\"\",\"\",IF(F2=D2,\"confirmed\",\"changed\"))\n",
    "## CELL H --> =IF(D2=F2,0,IF(D2>F2,0,IF(D2<F2,1)))\n",
    "## CELL I --> =IF(D2=F2,0,IF(D2<F2,0,IF(D2>F2,1)))\n",
    "\n",
    "# scores_csv = r\"Z:\\Proj_Yolo\\GobyFinder_GoPro_Model_Results\\Validation_edit\\yolov8x_GoPro_valid_test_scores.csv\"\n",
    "# save_excel = r\"Z:\\Proj_Yolo\\GobyFinder_GoPro_Model_Results\\Validation_edit\\yolov8x_GoPro_valid_test_scores_to_annot.xlsx\"\n",
    "\n",
    "# scores_csv = r\"Z:\\Proj_Yolo\\GobyFinder_GoPro_Model_Results\\Train_edit\\yolov8x_GoPro_train_test_scores.csv\"\n",
    "scores_csv = r\"C:\\Users\\ageglio\\ageglio-1\\gobyfinder_yolov8\\inference\\LMBS_Goby_Photos_conf0.05\\LMBS_Goby_Photos_conf0.05_Yolo_predictions.csv\"\n",
    "save_excel = r\"C:\\Users\\ageglio\\ageglio-1\\gobyfinder_yolov8\\inference\\LMBS_Goby_Photos_conf0.05\\LMBS_Goby_Photos_conf0.05_Yolo_predictions_QAQC.xlsx\"\n",
    "def scores_to_annot(scores_csv, scored=False):\n",
    "    scores_df = pd.read_csv(scores_csv, index_col=0)\n",
    "\n",
    "    if scored:\n",
    "        df_edit = scores_df[[\"filename\", \"detect_id\", \"tp\", \"fp\"]]\n",
    "        df_edit = df_edit.sort_values(by=\"detect_id\")\n",
    "        df_edit[\"CHECK\"] = np.where(df_edit.tp == 1, 1, np.nan)\n",
    "    else:\n",
    "         df_edit = scores_df[[\"filename\", \"detect_id\", \"conf\"]]\n",
    "         df_edit = df_edit.copy()\n",
    "         df_edit[\"path\"] = df_edit.filename.apply(lambda x: os.path.dirname(x.replace(\"_\", \"/\")))\n",
    "         df_edit[\"filename\"] = df_edit.filename.apply(lambda x: x.split(\"_\")[-1])\n",
    "         df_edit[\"detect_id\"] = df_edit.detect_id.apply(lambda x: x.split(\"_\")[-2]+\" \" + x.split(\"_\")[-1])\n",
    "        #  df_edit = df_edit.sort_values(by=\"detect_id\")\n",
    "         df_edit[\"CHECK\"] = np.nan\n",
    "    return df_edit\n",
    "scores_to_annot(scores_csv).to_excel(save_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. After annotation is complete, create the updated scores sheet to compare metrics\n",
    "\n",
    "scores_annoted_xls = r\"Z:\\Proj_Yolo\\GobyFinder_GoPro_Model_Results\\Test_edit\\yolov8x_GoPro_test_scores_to_annot.xlsx\"\n",
    "scores_csv = r\"test_runs\\detect\\yolov8x_GoPro_2_results\\yolov8x_GoPro_2_scores.csv\"\n",
    "labels_csv = r\"test_runs\\detect\\yolov8x_GoPro_2_results\\yolov8x_GoPro_2_Labels.csv\"\n",
    "## Updates scores based on the scores_to_annot.xlsx\n",
    "def scores_edited(scores_csv, scores_annoted_xls):\n",
    "    scores_df = pd.read_csv(scores_csv, index_col=0)\n",
    "    scored_annot_df = pd.read_excel(scores_annoted_xls, index_col=0)\n",
    "    df_lbls = pd.read_csv(labels_csv, index_col=0)\n",
    "    n_ground_truth = len(df_lbls)\n",
    "    scores_edited = pd.merge(scores_df, scored_annot_df[['detect_id', 'fp-tp', 'tp-fp']], on = 'detect_id')\n",
    "    scores_edited['tp_n'] = scores_edited.tp + scores_edited['fp-tp'] - scores_edited['tp-fp']\n",
    "    scores_edited['fp_n'] = scores_edited.fp - scores_edited['fp-tp'] + scores_edited['tp-fp']\n",
    "    scores_edited['acc_tp_n'] = scores_edited.tp_n.cumsum()\n",
    "    scores_edited['acc_fp_n'] = scores_edited.fp_n.cumsum()\n",
    "    scores_edited['Precision_n'] = scores_edited.acc_tp_n/(scores_edited.acc_tp_n + scores_edited.acc_fp_n)\n",
    "    n_ground_truth_n = n_ground_truth + (scores_edited.acc_tp_n.max() - scores_edited.acc_tp.max()) # new ground truth gobies base on annotation\n",
    "    scores_edited['Recall_n'] = scores_edited.acc_tp_n/n_ground_truth_n\n",
    "    name = os.path.basename(scores_annoted_xls)\n",
    "    folder = os.path.dirname(scores_annoted_xls)\n",
    "    name = str(name).split(\".\")[0]+\"_updated.csv\"\n",
    "    scores_edited_df = scores_edited\n",
    "    # scores_edited.to_csv(os.path.join(folder, name))\n",
    "    return n_ground_truth, n_ground_truth_n, scores_edited_df\n",
    "n_ground_truth, n_ground_truth_n, scores_edited_df = scores_edited(scores_csv, scores_annoted_xls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Calculate the change in metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_curves(df, labels=None, gt=0, gt_n=0):\n",
    "    TP = df.acc_tp.values\n",
    "    TPn = df.acc_tp_n.values\n",
    "    FN = gt - TP\n",
    "    FNn = gt_n - TPn\n",
    "    FP = df.acc_fp.values\n",
    "    FPn = df.acc_fp_n.values\n",
    "    x = df.conf.values\n",
    "    # m = df.miou.values\n",
    "    # f = df.F1_score.values\n",
    "    p = df.Precision\n",
    "    pi = [p[0]] + [max(p[i+1:]) for i in range(len(p)-1)]\n",
    "    pn = df.Precision_n\n",
    "    pin = [pn[0]] + [max(pn[i+1:]) for i in range(len(pn)-1)]\n",
    "    r = df.Recall\n",
    "    rn = df.Recall_n\n",
    "    f1 = 2*(p*r)/(p+r)\n",
    "    f1n = 2*(pn*rn)/(pn+rn)\n",
    "    maxf1 = f1.max()\n",
    "    maxf1n = f1n.max()\n",
    "    # a = (m+f)/2\n",
    "    fig, ax = plt.subplots(1, figsize = (6,4))\n",
    "\n",
    "    # ax.plot(x, r, label='Recall', linestyle='-')\n",
    "    # ax.plot(x, pi, label='Precision', linestyle='-')\n",
    "    # ax.plot(x, f1, marker='.', label=f'F1 score {maxf1:0.3f}', markersize=1)\n",
    "\n",
    "    ax.plot(x, rn, label='Recall_n', linestyle='--')\n",
    "    ax.plot(x, pin, label='Precision_n', linestyle='--')\n",
    "    # ax.plot(x, f1, marker='.', label=f'F1 score {maxf1:0.3f}', markersize=1)\n",
    "    \n",
    "    ax.set_ylabel('score')\n",
    "    ax.set_xlabel('Confidence threshold')\n",
    "\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    print(f\"max F1: {maxf1:0.2f}, num ground truth fish {n_ground_truth}\")\n",
    "    print(f\"new max F1: {maxf1n:0.2f}, num ground truth fish {n_ground_truth_n}\")\n",
    "plot_curves(scores_edited_df, gt = n_ground_truth, gt_n = n_ground_truth_n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolov8v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
