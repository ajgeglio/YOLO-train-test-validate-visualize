{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append(r\"..\\GobyFinder\")  # Replace with the actual path to the GobyFinder module\n",
    "from src import Utils, reports\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the goby collects only (all years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_goby_collects(op_table_pth, year):\n",
    "    op_table = pd.read_excel(op_table_pth) # \n",
    "    op_table_assmt = op_table[op_table.GOBY == 1]\n",
    "    df_goby_sites = op_table_assmt[op_table_assmt[\"COLLECT_ID\"].str.contains(f\"{year}\")]\n",
    "    return df_goby_sites['COLLECT_ID']\n",
    "## Generate metadata from all unpacked images\n",
    "def usable_goby_metadata(aluim, op_table_pth, year):\n",
    "    goby_collect_list = list_goby_collects(op_table_pth, year)\n",
    "    print(len(goby_collect_list), \"goby collects from\", year)\n",
    "    meta_df = aluim\n",
    "    print(\"original metadata\", meta_df.shape)\n",
    "    meta_potentially_usable = meta_df[(meta_df['Metadata Thresholds'] == \"Pass\")]\n",
    "    meta_usable_rf = meta_potentially_usable[(meta_potentially_usable['Usability Random Forest'] == \"Pass\")]\n",
    "    meta_usable = meta_df[(meta_df['Usability'] == \"Usable\")]\n",
    "    print(\"portion usable out of potential\", meta_usable_rf.shape[0]/meta_potentially_usable.shape[0])\n",
    "    meta_usable_goby = meta_usable[meta_usable.collect_id.isin(goby_collect_list)]\n",
    "    #Checks\n",
    "    orig_shape = meta_usable_goby.shape[0]\n",
    "    print(\"Goby collects metadata\", orig_shape)\n",
    "    drop_fnd = meta_usable_goby.drop_duplicates(subset=\"Filename\").shape[0]\n",
    "    print(orig_shape - drop_fnd, \"duplicate filenames\")\n",
    "    drop_fnn = meta_usable_goby.dropna(subset=\"Filename\").shape[0]\n",
    "    print(orig_shape - drop_fnn, \"missing filenames\")\n",
    "    drop_ipd = meta_usable_goby.drop_duplicates(subset=\"image_path\").shape[0]\n",
    "    print(orig_shape - drop_ipd, \"duplicate image paths\")\n",
    "    drop_tsn = meta_usable_goby.dropna(subset=\"Time_s\").shape[0]\n",
    "    print(orig_shape - drop_tsn, \"missing timestamps\")\n",
    "    drop_dbn = meta_usable_goby.dropna(subset=\"DistanceToBottom_m\").shape[0]\n",
    "    print(orig_shape - drop_dbn, \"missing altitudes\")\n",
    "    # dropping nan in DistanceToBottom_m\n",
    "    meta_usable_goby_dbn = meta_usable_goby.dropna(subset=\"DistanceToBottom_m\")\n",
    "    print(\"final table\", meta_usable_goby_dbn.shape)\n",
    "    ## median \n",
    "    medians = meta_usable_goby_dbn.groupby('collect_id')['Time_s'].apply(lambda x: x.diff()).values\n",
    "    print(\"Average median time interval\", np.nanmedian(medians))\n",
    "    return meta_usable_goby_dbn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_table_pth = r\"Z:\\__AdvancedTechnologyBackup\\01_DerivedProducts\\Database\\OP_TABLE.xlsx\"\n",
    "t = datetime.datetime.now()\n",
    "Ymmdd = f\"{t.year:02d}{t.month:02d}{t.day:02d}\"\n",
    "print(\"YYYYmmdd:\", Ymmdd)\n",
    "metadata = pd.read_pickle(r\"Z:\\__AdvancedTechnologyBackup\\01_DerivedProducts\\Database\\02__MetadataCombined\\all_unpacked_images_metadata.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "aluim = metadata[metadata.year == str(year)].copy()\n",
    "\n",
    "'''\n",
    "83 goby collects from 2024\n",
    "original metadata (200802, 169)\n",
    "portion usable out of potential 0.6926015422337515\n",
    "Goby collects metadata 108082\n",
    "0 duplicate filenames\n",
    "0 missing filenames\n",
    "0 duplicate image paths\n",
    "0 missing timestamps\n",
    "2522 missing altitudes\n",
    "final table (105560, 169)\n",
    "Average median time interval 5.0\n",
    "'''\n",
    "meta_2024_usable_gobydd = usable_goby_metadata(aluim, op_table_pth, 2024)\n",
    "# meta_2024_usable_gobydd.to_csv(r\"Z:\\Proj_Yolo\\2024_Yolo_update\\02_2024_Assessment\\meta_2024_usable_goby.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_2024_usable_gobydd[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_lst_2024 = list(meta_2024_usable_gobydd.image_path)\n",
    "filename = r\"Z:\\Proj_Yolo\\2024_Yolo_update\\02_2024_Assessment\\img_lst_2024.csv\"\n",
    "# general.write_list_txt(filename, img_lst_2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023, earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2023\n",
    "aluim = metadata[metadata.year == str(year)].copy()\n",
    "'''\n",
    "94 goby collects from 2023\n",
    "original metadata (278703, 158)\n",
    "portion usable out of potential 0.5728057392668984\n",
    "Goby collects metadata 109789\n",
    "0 duplicate filenames\n",
    "0 missing filenames\n",
    "0 duplicate image paths\n",
    "0 missing timestamps\n",
    "6112 missing altitudes\n",
    "final table (103677, 158)\n",
    "Average median time interval 4.999852895736694\n",
    "'''\n",
    "meta_2023_usable_gobydd = usable_goby_metadata(aluim, op_table_pth, 2023)\n",
    "# meta_2023_usable_gobydd.to_csv(r\"Z:\\Proj_Yolo\\2023_Yolo_update\\02_2023_Assessment\\meta_2023_usable_goby2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "aluim = metadata[metadata.year == str(year)].copy()\n",
    "\n",
    "'''\n",
    "original metadata (241828, 145)\n",
    "Drop unusable (113769, 145)\n",
    "Goby collects metadata (107651, 145)\n",
    "drop filename nan (107651, 145)\n",
    "drop filename dup (107651, 145)\n",
    "drop Time_s nan (107651, 145)\n",
    "drop DistanceToBottom_m nan (100085, 145)\n",
    "drop image_path dup (107651, 145)\n",
    "final table (100085, 145)\n",
    "Average median time interval 4.9998345375061035\n",
    "'''\n",
    "meta_2022_usable_gobydd = usable_goby_metadata(aluim, op_table_pth, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aluim = r\"Z:\\Database\\02__MetadataCombined\\all_unpacked_images_metadata_2021.csv\"\n",
    "'''\n",
    "83 goby collects from 2021\n",
    "original metadata (275149, 156)\n",
    "portion usable out of potential 0.4529760084512524\n",
    "Goby collects metadata 86275\n",
    "0 duplicate filenames\n",
    "0 missing filenames\n",
    "0 duplicate image paths\n",
    "0 missing timestamps\n",
    "556 missing altitudes\n",
    "final table (85719, 156)\n",
    "Average median time interval 4.019714117050171\n",
    "'''\n",
    "meta_2021_usable_gobydd = usable_goby_metadata(aluim,op_table_pth, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aluim = r\"Z:\\Database\\02__MetadataCombined\\all_unpacked_images_metadata_2020.csv\"\n",
    "'''\n",
    "original metadata (112808, 123)\n",
    "Drop unusable (51346, 123)\n",
    "Goby collects metadata (51346, 123)\n",
    "drop filename nan (51346, 123)\n",
    "drop filename dup (51346, 123)\n",
    "drop Time_s nan (51346, 123)\n",
    "drop DistanceToBottom_m nan (50380, 123)\n",
    "drop image_path dup (51346, 123)\n",
    "final table (50380, 123)\n",
    "Average median time interval 4.0\n",
    "'''\n",
    "meta_2020_usable_gobydd = usable_goby_metadata(aluim, op_table_pth, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_2020_usable_gobydd.to_csv(os.path.join(r\"Z:\\Proj_Yolo\\2020_Yolo_update\\02_2020_Assessment\\2020_Assessment_11-2\", f\"meta_2020_usable_goby_{Ymmdd}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## April - May shallow study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2 = ['Time_s', 'Filename', 'Latitude', 'Longitude', 'DepthFromSurface_m', 'DistanceToBottom_m', 'TotalWaterColumn_m','Speed_kn', 'Time_UTC', 'DataSource',\n",
    "            'CollectID', 'BagFile', 'CameraRoll','Usability Random Forest', 'Usability', 'collect_id', 'PointCloudStandardDeviation', 'PointCloudElevation_m', 'year','month', 'day', 'time', 'imw', 'imh', 'image_path']\n",
    "# all_unpacked = pd.read_pickle(r\"Z:\\Database\\02__MetadataCombined\\all_unpacked_images_metadata.pickle\")[columns2]\n",
    "# au_april_may = all_unpacked[(all_unpacked.Usability==\"Usable\") & (all_unpacked.month.isin([4,5])) & (all_unpacked.DepthFromSurface_m<30)]\n",
    "# au_april_may.to_csv(r\"inference\\april-may-shallow\\april-may-shallow-metadata.csv\")\n",
    "# au_april_may_lst = list(au_april_may.image_path)\n",
    "# filename = r\"inference\\april-may-shallow\\april-may-shallow-metadata_lst.csv\"\n",
    "# general.write_list_txt(filename, au_april_may_lst)\n",
    "# au_april_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Test Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_set(runs, df_20_23_alln):\n",
    "    dev_all = pd.DataFrame()    \n",
    "    for run in runs:\n",
    "        rt = pd.read_csv(f\"datasets\\\\AUV_datasets\\\\Runs\\\\{run}\\\\train_df.csv\", index_col=0)\n",
    "        rf = pd.read_csv(f\"datasets\\\\AUV_datasets\\\\Runs\\\\{run}\\\\valid_df.csv\", index_col=0)\n",
    "        dev_all = pd.concat([dev_all, rt, rf])\n",
    "    df = df_20_23_alln[~df_20_23_alln.image_id.isin(dev_all.Filename)]\n",
    "    min_year_count = df.year.value_counts().min()\n",
    "    balanced_test_df = df.groupby('year', group_keys=False).apply(lambda x: x.sample(min_year_count, random_state=123))\n",
    "    return balanced_test_df.reset_index()\n",
    "\n",
    "def copy_balanced_test(balanced_test_df, year):\n",
    "    balanced_test_year_df = balanced_test_df[balanced_test_df.year == year]\n",
    "    l = len(balanced_test_year_df)\n",
    "    for idx, row in balanced_test_year_df.iterrows():\n",
    "        index, image_path, Filename, image_id, bbox_path, mer_path, year, n_fish = row\n",
    "        dest1 = f\"datasets\\\\AUV_datasets\\\\test_sets\\\\balanced_test\\\\{year}\\\\images\"\n",
    "        dest2 = f\"datasets\\\\AUV_datasets\\\\test_sets\\\\balanced_test\\\\{year}\\\\labels\"\n",
    "        if not os.path.exists(dest1): os.makedirs(dest1)\n",
    "        if not os.path.exists(dest2): os.makedirs(dest2)\n",
    "        dest_image_path = os.path.join(dest1, os.path.basename(image_path))\n",
    "        dest_bbox_path = os.path.join(dest2, os.path.basename(bbox_path))\n",
    "        if not os.path.exists(dest_image_path):\n",
    "            shutil.copy2(image_path, dest_image_path)\n",
    "            print(idx, \"/\", l, end = \"   \\r\")\n",
    "        if not os.path.exists(dest_bbox_path):\n",
    "            shutil.copy2(bbox_path, dest_bbox_path)\n",
    "            print(idx, \"/\", l, end = \"   \\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_23_alln = pd.read_csv(r\"datasets\\AUV_datasets\\All_Goby_imgs_lbls_pths_n_fish.csv\", index_col=0)\n",
    "# A test set that does not include any of the previous development sets in given runs\n",
    "balanced_test_df = balanced_set(runs=[\"run12\"], df_20_23_alln= df_20_23_alln)\n",
    "\n",
    "# Check if the balanced_test_df dataframe has the expected structure and values\n",
    "expected_image_ids = [\"PI_1598268549_144_Iver3069\", \"PI_1663618220_651_Iver3069\", \"PI_1660504715_790_Iver3098\"]\n",
    "assert all(balanced_test_df.loc[idx, \"image_id\"] == expected_id for idx, expected_id in zip([10, 2355, 2364], expected_image_ids)), \"Dataframe does not match expected values\"\n",
    "\n",
    "# Check if the dataframe has the expected number of rows and columns\n",
    "expected_shape = (3336, 8)\n",
    "assert balanced_test_df.shape == expected_shape, f\"Expected shape {expected_shape}, but got {balanced_test_df.shape}\"\n",
    "print(balanced_test_df.shape)\n",
    "'''year\n",
    "2020    824\n",
    "2021    824\n",
    "2022    824\n",
    "2023    824'''\n",
    "print(balanced_test_df.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "copy_balanced_test(balanced_test_df, year=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_imgs_metadata = pd.read_csv(r\"Z:\\Database\\02__MetadataCombined\\all_unpacked_images_metadata_annotated.csv\", index_col=0, low_memory=False)\n",
    "balanced_test_metadata_df = annotated_imgs_metadata[annotated_imgs_metadata.Filename.isin(balanced_test_df.image_id)]\n",
    "assert balanced_test_metadata_df.shape[0] == balanced_test_df.shape[0]\n",
    "columns2 = ['Time_s', 'Filename', 'Latitude', 'Longitude', 'DepthFromSurface_m', 'DistanceToBottom_m', 'TotalWaterColumn_m','Speed_kn', 'Time_UTC', 'DataSource', 'InvalidTelnetLines',\n",
    "       'CollectID', 'BagFile', 'CameraRoll','Usability Random Forest', 'Usability', 'collect_id', 'PointCloudStandardDeviation', 'PointCloudElevation_m', 'year','month', 'day', 'time', 'imw', 'imh', 'image_path']\n",
    "balanced_test_metadata_df = balanced_test_metadata_df[columns2]\n",
    "# balanced_test_metadata_df.to_csv(r\"C:\\Users\\ageglio\\ageglio-1\\gobyfinder_yolov8\\inference\\Balanced_test_init_assmt\\balanced_test_metadata_df.csv\")\n",
    "# filename = r\"C:\\Users\\ageglio\\ageglio-1\\gobyfinder_yolov8\\inference\\Balanced_test_init_assmt\\balanced_test_metadata_lst.csv\"\n",
    "# general.write_list_txt(filename, balanced_test_metadata_df.image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running YOLO_predict with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thresh = 0.43\n",
    "df_pred = pd.read_csv(r\"test_runs\\detect\\balanced_test_init_assmt\\balanced_test_init_assmt_predictions.csv\", index_col=0)\n",
    "df_lbls = pd.read_csv(r\"test_runs\\detect\\balanced_test_init_assmt\\balanced_test_init_assmt_Labels.csv\", index_col=0)\n",
    "fn_df = reports().return_fn_df(df_lbls, df_pred, iou_tp = 0.5, conf_thresh = conf_thresh)\n",
    "scores_df = reports().scores_df(df_lbls, df_pred, iou_tp = 0.5)\n",
    "## FP dataframe\n",
    "fp_scores = scores_df[(scores_df.fp==1) & (scores_df.conf>=conf_thresh)]\n",
    "fp_scores = fp_scores.rename(columns = {\"x_p\":\"x\", \"y_p\":\"y\", \"w_p\":\"w\", \"h_p\":\"h\"})\n",
    "fp_scores.to_csv(r\"inference\\Balanced_test_init_assmt\\balanced_fp_scores.csv\")\n",
    "## TP dataframe\n",
    "tp_scores = scores_df[(scores_df.tp==1) & (scores_df.conf>=conf_thresh)]\n",
    "tp_scores = tp_scores.rename(columns = {\"x_p\":\"x\", \"y_p\":\"y\", \"w_p\":\"w\", \"h_p\":\"h\"})\n",
    "tp_scores.to_csv(r\"inference\\Balanced_test_init_assmt\\balanced_tp_scores.csv\")\n",
    "## FN dataframe\n",
    "## Detections have to be pre-filtered by confidence threshold for analysis of false negatives\n",
    "fn_df = fn_df[(fn_df.fn==1)]\n",
    "fn_df = fn_df.rename(columns = {\"x_l\":\"x\", \"y_l\":\"y\", \"w_l\":\"w\", \"h_l\":\"h\"})\n",
    "fn_df.to_csv(r\"inference\\Balanced_test_init_assmt\\balanced_fn_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def freq_dist_month(df, ax, label, color, position):\n",
    "    unix_times = df['Time_s'].values\n",
    "    dates = [datetime.utcfromtimestamp(ts) for ts in unix_times]\n",
    "    date_df = pd.DataFrame(dates, columns=['date'])\n",
    "    date_df['month'] = date_df['date'].dt.to_period('M')\n",
    "    monthly_counts = date_df['month'].value_counts().sort_index()\n",
    "    monthly_counts.index = monthly_counts.index.to_timestamp()\n",
    "    ax.bar(monthly_counts.index + pd.DateOffset(days=position), monthly_counts.values, width=20, label=label, color=color)\n",
    "    for date, count in monthly_counts.items():\n",
    "        if date.month == 9:\n",
    "            ax.annotate(f'{date.year}', xy=(date, count), xytext=(0, 5), textcoords='offset points', ha='center', fontsize=8, color=color)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "freq_dist_month(annotated_imgs_metadata, ax, label='Annotated Images', color='blue', position=0)\n",
    "freq_dist_month(balanced_test_metadata_df, ax, label='Balanced Test', color='orange', position=0)\n",
    "\n",
    "# Add vertical dotted lines for January\n",
    "years = annotated_imgs_metadata['Time_s'].apply(lambda x: datetime.utcfromtimestamp(x).year).unique()\n",
    "for year in years:\n",
    "    jan = datetime(year, 1, 1)\n",
    "    ax.axvline(jan, color='gray', linestyle='dotted')\n",
    "\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Number of Samples')\n",
    "ax.set_title('Frequency Distribution of Samples per Month')\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "ax.set_yscale('log')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
