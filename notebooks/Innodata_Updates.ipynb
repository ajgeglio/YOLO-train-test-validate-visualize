{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746fb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"src\")))\n",
    "from utils import Utils\n",
    "from labelUtils import LabelUtils\n",
    "from reportFunctions import Reports\n",
    "import matplotlib.pyplot as plt\n",
    "from overlayFunctions import Overlays\n",
    "from utils import Utils\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def zip_batch_pred_labels(batch_folder, zip_it=True):\n",
    "    lbl_folder = os.path.join(batch_folder, \"yolo_labels\")\n",
    "    if zip_it and os.path.exists(lbl_folder):\n",
    "        Utils.zip_folder(lbl_folder)\n",
    "    return lbl_folder\n",
    "\n",
    "def copy_zip_batch_gt_labels(batch_folder, image_list, zip_it=True):\n",
    "    lbl_paths = []\n",
    "    for img_path in image_list:\n",
    "        p = Path(img_path)\n",
    "\n",
    "        lbl_p = p.parents[1] / \"labels\" / p.with_suffix('.txt').name\n",
    "        lbl_paths.append(str(lbl_p))\n",
    "\n",
    "    lbl_folder = os.path.join(batch_folder, \"gt_labels\")\n",
    "    os.makedirs(lbl_folder, exist_ok=True)\n",
    "    \n",
    "    # Using your helper to copy the files\n",
    "    Utils.copy_files_lst(lbl_paths, lbl_folder)\n",
    "    \n",
    "    if zip_it:\n",
    "        Utils.zip_folder(lbl_folder)\n",
    "    return lbl_folder\n",
    "\n",
    "def copy_zip_batch_images_labels(batch_folder, zip_it=True, gt=False):\n",
    "    # 1. Load image list\n",
    "    image_list = Utils.read_list_txt(os.path.join(batch_folder, \"images.txt\"))\n",
    "    img_folder = os.path.join(batch_folder, \"images\")\n",
    "    os.makedirs(img_folder, exist_ok=True)\n",
    "    \n",
    "    # 2. Copy images\n",
    "    Utils.copy_files_lst(image_list, img_folder)\n",
    "    \n",
    "    # 3. Handle Labels\n",
    "    if not gt:\n",
    "        # Just use the existing yolo_labels generated by previous step\n",
    "        target_lbl_folder = os.path.join(batch_folder, \"yolo_labels\")\n",
    "    else:\n",
    "        # Copy GT labels from source\n",
    "        target_lbl_folder = copy_zip_batch_gt_labels(batch_folder, image_list, zip_it=zip_it)\n",
    "\n",
    "    # 4. Assertion (Must happen BEFORE zipping/deleting)\n",
    "    # Note: This will still fail if you have background images with no GT txt files\n",
    "    img_count = len(os.listdir(img_folder))\n",
    "    lbl_count = len(os.listdir(target_lbl_folder))\n",
    "    print(f\"Batch {batch_folder}: Images({img_count}) vs Labels({lbl_count})\")\n",
    "    \n",
    "    # 5. Cleanup and Zipping\n",
    "    if zip_it:\n",
    "        Utils.zip_folder(img_folder)\n",
    "        Utils.zip_folder(target_lbl_folder)\n",
    "        \n",
    "        # Only remove if we actually zipped them\n",
    "        shutil.rmtree(img_folder)\n",
    "        shutil.rmtree(target_lbl_folder)\n",
    "        \n",
    "        # If we created gt_labels and yolo_labels was also there, clean up yolo_labels\n",
    "        pred_folder = os.path.join(batch_folder, \"yolo_labels\")\n",
    "        if gt and os.path.exists(pred_folder):\n",
    "            shutil.rmtree(pred_folder)\n",
    "            \n",
    "def clean_empty_images_from_batch(batch_folder):\n",
    "    pred_zip_path = os.path.join(batch_folder, \"yolo_labels.zip\")\n",
    "    gt_zip_path = os.path.join(batch_folder, \"gt_labels.zip\")\n",
    "    img_zip_path = os.path.join(batch_folder, \"images.zip\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not all(os.path.exists(f) for f in [pred_zip_path, gt_zip_path, img_zip_path]):\n",
    "        print(f\"   Skipping {batch_folder}: Missing ZIPs.\")\n",
    "        return\n",
    "\n",
    "    valid_stems = set()\n",
    "\n",
    "    # 1. Identify \"Valid\" images (non-empty labels)\n",
    "    for zip_path in [pred_zip_path, gt_zip_path]:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            for info in z.infolist():\n",
    "                if info.filename.endswith('.txt') and info.file_size > 0:\n",
    "                    # Use Path(info.filename).stem to ignore folder names in zip\n",
    "                    valid_stems.add(Path(info.filename).stem)\n",
    "\n",
    "    print(f\"   Identified {len(valid_stems)} images with annotations.\")\n",
    "\n",
    "    # 2. Extract and Filter Images\n",
    "    temp_img_folder = os.path.join(batch_folder, \"temp_images_extract\")\n",
    "    if os.path.exists(temp_img_folder): shutil.rmtree(temp_img_folder)\n",
    "    os.makedirs(temp_img_folder, exist_ok=True)\n",
    "    \n",
    "    with zipfile.ZipFile(img_zip_path, 'r') as z:\n",
    "        z.extractall(temp_img_folder)\n",
    "    \n",
    "    # Walk through extracted files (handles nested folders safely)\n",
    "    final_image_paths = []\n",
    "    for root, dirs, files in os.walk(temp_img_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_stem = Path(file).stem\n",
    "            \n",
    "            if file_stem not in valid_stems:\n",
    "                os.remove(file_path)\n",
    "            else:\n",
    "                final_image_paths.append(file_stem)\n",
    "\n",
    "    # 3. Re-zip and Cleanup\n",
    "    os.remove(img_zip_path) # Delete old zip\n",
    "    Utils.zip_folder(temp_img_folder) \n",
    "    \n",
    "    new_zip_name = temp_img_folder + \".zip\"\n",
    "    if os.path.exists(new_zip_name):\n",
    "        os.rename(new_zip_name, img_zip_path)\n",
    "    shutil.rmtree(temp_img_folder)\n",
    "\n",
    "    # 4. Update images.txt and labels.txt\n",
    "    # We read the existing ones to keep the path formatting (drive letters, etc.)\n",
    "    # but filter them by the stems we kept.\n",
    "    \n",
    "    for txt_file in [\"images.txt\", \"labels.txt\"]:\n",
    "        path = os.path.join(batch_folder, txt_file)\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Keep line if the filename stem is in our valid_stems\n",
    "            filtered_lines = [l for l in lines if Path(l.strip()).stem in valid_stems]\n",
    "            \n",
    "            with open(path, 'w') as f:\n",
    "                f.writelines(filtered_lines)\n",
    "                \n",
    "    print(f\"   Done. Kept {len(final_image_paths)} images and updated .txt lists.\")\n",
    "\n",
    "\n",
    "def create_shared_labels_for_batch(batch_folder):\n",
    "    img_zip_path = os.path.join(batch_folder, \"images.zip\")\n",
    "    gt_zip_path = os.path.join(batch_folder, \"gt_labels.zip\")\n",
    "    pred_zip_path = os.path.join(batch_folder, \"yolo_labels.zip\")\n",
    "    \n",
    "    if not os.path.exists(img_zip_path):\n",
    "        return\n",
    "\n",
    "    # 1. Get the list of images actually present in images.zip\n",
    "    current_image_stems = set()\n",
    "    with zipfile.ZipFile(img_zip_path, 'r') as z:\n",
    "        for name in z.namelist():\n",
    "            if not name.endswith('/'): # Skip directories\n",
    "                current_image_stems.add(Path(name).stem)\n",
    "\n",
    "    # 2. Setup temporary extraction directories\n",
    "    temp_gt = os.path.join(batch_folder, \"temp_gt\")\n",
    "    temp_pred = os.path.join(batch_folder, \"temp_pred\")\n",
    "    share_labels_dir = os.path.join(batch_folder, \"share_labels\")\n",
    "    \n",
    "    for d in [temp_gt, temp_pred, share_labels_dir]:\n",
    "        if os.path.exists(d): shutil.rmtree(d)\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # 3. Extract source labels\n",
    "    if os.path.exists(gt_zip_path):\n",
    "        with zipfile.ZipFile(gt_zip_path, 'r') as z:\n",
    "            z.extractall(temp_gt)\n",
    "            \n",
    "    if os.path.exists(pred_zip_path):\n",
    "        with zipfile.ZipFile(pred_zip_path, 'r') as z:\n",
    "            z.extractall(temp_pred)\n",
    "\n",
    "    # 4. Perform Prioritized Copy\n",
    "    # Strategy: GT first, then Supplement with Pred\n",
    "    count_gt = 0\n",
    "    count_pred = 0\n",
    "\n",
    "    for stem in current_image_stems:\n",
    "        target_name = f\"{stem}.txt\"\n",
    "        found = False\n",
    "        \n",
    "        # Look in GT first (Check for file existence and content)\n",
    "        gt_file = None\n",
    "        # Walk temp_gt in case of nested structure\n",
    "        for root, _, files in os.walk(temp_gt):\n",
    "            if target_name in files:\n",
    "                gt_file = os.path.join(root, target_name)\n",
    "                break\n",
    "        \n",
    "        if gt_file and os.path.getsize(gt_file) > 0:\n",
    "            shutil.copy2(gt_file, os.path.join(share_labels_dir, target_name))\n",
    "            count_gt += 1\n",
    "            found = True\n",
    "            \n",
    "        # If not found in GT, look in Pred\n",
    "        if not found:\n",
    "            pred_file = None\n",
    "            for root, _, files in os.walk(temp_pred):\n",
    "                if target_name in files:\n",
    "                    pred_file = os.path.join(root, target_name)\n",
    "                    break\n",
    "            \n",
    "            if pred_file and os.path.getsize(pred_file) > 0:\n",
    "                shutil.copy2(pred_file, os.path.join(share_labels_dir, target_name))\n",
    "                count_pred += 1\n",
    "                found = True\n",
    "        \n",
    "        # If still not found, create empty file to maintain 1:1 image-label ratio\n",
    "        if not found:\n",
    "            open(os.path.join(share_labels_dir, target_name), 'a').close()\n",
    "\n",
    "    # 5. Zip and Cleanup\n",
    "    Utils.zip_folder(share_labels_dir)\n",
    "    \n",
    "    # Verify zip exists before rmtree\n",
    "    if os.path.exists(share_labels_dir + \".zip\"):\n",
    "        shutil.rmtree(share_labels_dir)\n",
    "    \n",
    "    shutil.rmtree(temp_gt)\n",
    "    shutil.rmtree(temp_pred)\n",
    "\n",
    "    print(f\"   Success: {count_gt} GT labels, {count_pred} Pred labels used.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16442324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execution ---\n",
    "high_conf_batch_folders = [\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_001_conf_0.83\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_002_conf_0.58\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_003_conf_0.52\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_004_conf_0.48\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_005_conf_0.45\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_006_conf_0.42\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_007_conf_0.39\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_008_conf_0.37\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_009_conf_0.34\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_010_conf_0.32\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_011_conf_0.30\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_012_conf_0.29\",\n",
    "r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_013_conf_0.27\",\n",
    "]\n",
    "# for batch in high_conf_batch_folders:\n",
    "#     copy_zip_batch_images_labels(batch, zip_it=True, gt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ca64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## At and below confidence=0.25 we start having more discrepancy between the labels and the predictions so we priorize ground truths\n",
    "low_conf_batch_folders = [\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_014_conf_0.25\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_015_conf_0.24\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_016_conf_0.22\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_017_conf_0.21\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_018_conf_0.20\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_019_conf_0.19\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_020_conf_0.18\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_021_conf_0.17\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_022_conf_0.16\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_023_conf_0.15\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_024_conf_0.14\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_025_conf_0.13\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_026_conf_0.12\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_027_conf_0.12\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_028_conf_0.11\",\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\HNM_batch_029_conf_0.11\",\n",
    "]\n",
    "for batch in low_conf_batch_folders:\n",
    "    pass\n",
    "    # print(f\"Copying original labels and images from {batch}...\")\n",
    "    # copy_zip_batch_images_labels(batch_folder, zip_it=True, gt=True)\n",
    "    # print(f\"Processing {batch}...\")\n",
    "    # clean_empty_images_from_batch(batch)\n",
    "    # print(f\"Merging Labels for {batch}...\")\n",
    "    # create_shared_labels_for_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a48d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meta_nf = pd.read_csv(r\"Z:\\__AdvancedTechnologyBackup\\07_Database\\MetadataCombined\\all_annotated_meta_splits_filtered_20251030.csv\", index_col=0, low_memory=False)\n",
    "# filter for images with n_fish >= n images\n",
    "df_abs2_meta_nf = all_meta_nf[(all_meta_nf.n_fish >= 0)  & (all_meta_nf.imh > 2176)]\n",
    "df_abs1_meta_nf = all_meta_nf[(all_meta_nf.n_fish >= 0)  & (all_meta_nf.imh == 2176)]\n",
    "abiss2_len = len(df_abs2_meta_nf) \n",
    "abiss1_len = len(df_abs1_meta_nf)  \n",
    "# export_paths(df_abs1_meta_nf[df_abs1_meta_nf.split == \"test\"], run13_folder, subfolder=\"abiss1_test\")\n",
    "print(\"usable Abiss1 images:\", abiss1_len)\n",
    "print(\"usable Abiss2 images:\", abiss2_len) #usable Abiss2 images: 14225\n",
    "few_fish_df = df_abs2_meta_nf[(df_abs2_meta_nf.n_fish < 2)]\n",
    "new_path = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\one_fish\\images\"\n",
    "output_dir = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\one_fish\\overlays\"\n",
    "# Overlays.plot_label_overlays(few_fish_df.image_path, few_fish_df.label_path, output_dir, overwrite=False)\n",
    "# usable Abiss1 images: 33356\n",
    "# usable Abiss2 images: 13549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77359cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles_paths_from_tile_names(TILE_NAMES, BASE_DIR):\n",
    "    all_tiled_image_paths, all_tiled_label_paths = Utils.get_all_img_lbl_pths(BASE_DIR)\n",
    "    img_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_tiled_image_paths}\n",
    "    lbl_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_tiled_label_paths}\n",
    "    tiled_im_no_ext = list(map(lambda x: x.split(\".\")[0], TILE_NAMES))\n",
    "    batch_tiled_im_fp = [img_path_map_no_ext[bn] for bn in tiled_im_no_ext if bn in img_path_map_no_ext]\n",
    "    batch_tiled_lb_fp = [lbl_path_map_no_ext[bn] for bn in tiled_im_no_ext if bn in lbl_path_map_no_ext]\n",
    "    return sorted(batch_tiled_im_fp), sorted(batch_tiled_lb_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles_paths_from_batch_dir(BATCH_DIR, BASE_DIR, SPLITS):\n",
    "    all_tiled_image_paths, all_tiled_label_paths = Utils.get_all_img_lbl_pths(BASE_DIR, SPLITS)\n",
    "    batch_tiled_im = os.listdir(BATCH_DIR+\"\\\\\"+\"images\")\n",
    "    batch_tiled_lb = os.listdir(BATCH_DIR+\"\\\\\"+\"labels\")\n",
    "    batch_tiled_im_no_ext = list(map(lambda x: x.split(\".\")[0], batch_tiled_im))\n",
    "    batch_tiled_lb_no_ext = list(map(lambda x: x.split(\".\")[0], batch_tiled_lb))\n",
    "\n",
    "    # --- Refactored & Completed Assignments ---\n",
    "\n",
    "    # 1. Create basename-to-fullpath dictionaries (The key step for efficient lookup)\n",
    "    # Note: zip is often cleaner than map(lambda...) for pairing elements.\n",
    "    img_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_tiled_image_paths}\n",
    "    lbl_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_tiled_label_paths}\n",
    "\n",
    "    # 2. Use list comprehensions to look up the full paths using the batch basenames\n",
    "    # This is much faster than searching a list repeatedly.\n",
    "\n",
    "    batch_tiled_im_fp = [img_path_map_no_ext[bn] for bn in batch_tiled_im_no_ext if bn in img_path_map_no_ext]\n",
    "    batch_tiled_lb_fp = [lbl_path_map_no_ext[bn] for bn in batch_tiled_lb_no_ext if bn in lbl_path_map_no_ext]\n",
    "\n",
    "    # --- Verification (Optional) ---\n",
    "    print(f\"Total full paths found for images in batch: {len(batch_tiled_im_fp)}\")\n",
    "    print(f\"Total full paths found for labels in batch: {len(batch_tiled_lb_fp)}\")\n",
    "    # assert len(batch_tiled_im_fp) == len(batch_tiled_im), \"Some image files not found in the master list.\"\n",
    "    # assert len(batch_tiled_lb_fp) == len(batch_tiled_lb), \"Some label files not found in the master list.\"\n",
    "    return sorted(batch_tiled_im_fp), sorted(batch_tiled_lb_fp)\n",
    "\n",
    "# BATCH_DIR = r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\transects\\20240804_001_Iver3069_ABS2\\20240804_001_Iver3069_ABS2_batch_00\"\n",
    "# BASE_DIR = r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\transects\\20240804_001_Iver3069_ABS2\"\n",
    "# batch_tiled_im_fp, batch_tiled_lb_fp = get_tiles_paths_from_batch_dir(BATCH_DIR = BATCH_DIR, BASE_DIR = BASE_DIR, SPLITS = None)\n",
    "\n",
    "# Utils.write_list_txt(sorted(batch_tiled_im_fp), BATCH_DIR+\"\\\\\"+\"images.txt\")\n",
    "# Utils.write_list_txt(sorted(batch_tiled_lb_fp), BATCH_DIR+\"\\\\\"+\"labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6154d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_paths_from_tiled_images(tiled_images):\n",
    "    batch_set = set(map(lambda x: Utils.convert_tile_img_pth_to_basename(x), tiled_images))\n",
    "    all_full_imgs = glob.glob(r\"D:\\datasets\\full\\*\\images\\*.png\")\n",
    "    all_full_imgs += glob.glob(r\"D:\\datasets\\full\\*\\images\\*.jpg\")\n",
    "    all_full_lbls = glob.glob(r\"D:\\datasets\\full\\*\\labels\\*.txt\")\n",
    "    print(\"full images\", len(all_full_imgs))\n",
    "    print(\"full labels\", len(all_full_lbls))\n",
    "    assert len(all_full_imgs) == len(all_full_lbls)\n",
    "    img_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_full_imgs}\n",
    "    lbl_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_full_lbls}\n",
    "    batch_full_im_fp = [img_path_map_no_ext[bn] for bn in batch_set if bn in img_path_map_no_ext]\n",
    "    batch_full_lb_fp = [lbl_path_map_no_ext[bn] for bn in batch_set if bn in lbl_path_map_no_ext]\n",
    "    assert len(batch_full_im_fp) == len(batch_full_lb_fp), \"number of images and labels do not match\"\n",
    "    print(\"full image paths from tiled paths\", len(batch_full_lb_fp))\n",
    "    return batch_full_im_fp, batch_full_lb_fp\n",
    "    \n",
    "# batch_full_im_fp, batch_full_lb_fp = get_full_paths_from_tiled_images(batch_tiled_im_fp)   \n",
    "# Utils.write_list_txt(batch_full_im_fp, r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\\2025_goby_relabel_batch_01\\full\\images.txt\")\n",
    "# Utils.write_list_txt(batch_full_lb_fp, r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\\2025_goby_relabel_batch_01\\full\\labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d1ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conf_batches(scores_df: pd.DataFrame, pred_df: pd.DataFrame, output_directory: str, batch_size: int = 1500):\n",
    "    \"\"\"\n",
    "    Bins the dataframe samples into batches of EXACTLY 'batch_size' unique images (except the final batch).\n",
    "    \"\"\"\n",
    "    print(f\"Starting batch creation for {len(scores_df['Filename'].unique())} unique images based on scores_df...\")\n",
    "    \n",
    "    # 1. Sort the scores DataFrame by 'conf'\n",
    "    df_scored_sorted = scores_df.sort_values(by='conf', ascending=False)\n",
    "    \n",
    "    # Get the unique image filenames in the sorted order as a NumPy array for clean splitting\n",
    "    unique_filenames_array = df_scored_sorted['Filename'].drop_duplicates().to_numpy()\n",
    "    num_unique_images = len(unique_filenames_array)\n",
    "    \n",
    "    # 2. Determine batch splits\n",
    "    # Calculate the exact number of batches required\n",
    "    num_batches = int(np.ceil(num_unique_images / batch_size))\n",
    "\n",
    "    # --- CORE FIX ---\n",
    "    # Use array_split to divide the unique_filenames_array into 'num_batches' parts.\n",
    "    # This guarantees that the first (N-1) parts are as close to 'batch_size' as possible.\n",
    "    # When num_batches is calculated based on batch_size, this ensures the size target.\n",
    "    batch_filename_splits = np.array_split(unique_filenames_array, num_batches)\n",
    "    # ----------------\n",
    "    \n",
    "    base_output_path = Path(output_directory)\n",
    "    \n",
    "    # 3. Iterate through batches\n",
    "    for i, batch_filenames in enumerate(batch_filename_splits):\n",
    "        \n",
    "        # 'batch_filenames' is now an array containing the exact filenames for this batch.\n",
    "        \n",
    "        # Select the hard negative rows for the batch for scoring/metadata\n",
    "        scores_df_isin = df_scored_sorted[df_scored_sorted['Filename'].isin(batch_filenames)].copy()\n",
    "        \n",
    "        # Select ALL prediction rows for the batch for YOLO label generation\n",
    "        batch_df_isin = pred_df[pred_df['Filename'].isin(batch_filenames)].copy()\n",
    "        \n",
    "        # 4. Determine confidence range of the FP for naming\n",
    "        conf_min = scores_df_isin['conf'].min()\n",
    "        conf_max = scores_df_isin['conf'].max()\n",
    "        \n",
    "        # 5. Create batch output directory\n",
    "        folder_name = f\"HNM_batch_{i+1:03d}_conf_{conf_max:.2f}\"\n",
    "        batch_output_path = base_output_path / folder_name\n",
    "        os.makedirs(batch_output_path, exist_ok=True)\n",
    "\n",
    "        # Save the hard negative dataframe for this specific batch\n",
    "        scores_df_isin.to_csv(os.path.join(batch_output_path, \"scores_df.csv\"), index=False)\n",
    "        \n",
    "        # 6. Save image list file (images.txt)\n",
    "        image_paths = batch_df_isin['tile_path'].unique().tolist()\n",
    "      \n",
    "        # image_list_filename = batch_output_path / \"images.txt\"\n",
    "        # Utils.write_list_txt(image_paths, str(image_list_filename))\n",
    "        \n",
    "        # 7. Save YOLO prediction labels for the batch (assuming LabelUtils is fixed to accept confidence_thresh)\n",
    "        LabelUtils.convert_predict_df_to_yolo_labels(batch_df_isin, confidence_thresh=0.25, batch_output_dir=str(batch_output_path))\n",
    "        \n",
    "        # Print actual size of the batch\n",
    "        print(f\"✅ Batch {i+1}/{num_batches} (Images: {len(image_paths)}, Conf Range: [{conf_min:.2f}, {conf_max:.2f}]) created in: {batch_output_path}\", end=\"  \\r\")\n",
    "\n",
    "    print(\"\\nBatch creation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9467323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HNM tiles - Tiles with False positive predictions with high confidence\n",
    "# thresh = 0.1\n",
    "# run13_HNM_test_score = pd.read_csv(r\"D:\\ageglio-1\\gobyfinder_yolov8\\output\\test_runs\\run13-tile-test\\scores.csv\", index_col=0)\n",
    "\n",
    "# run13_HNM_test_score_fp = run13_HNM_test_score[run13_HNM_test_score.fp==1]\n",
    "# HNM_fp_objects = run13_HNM_test_score_fp[run13_HNM_test_score_fp.conf>=thresh].copy()\n",
    "# HNM_fp_tiles = HNM_fp_objects.Filename.unique()\n",
    "# HNM_fp_full_images = HNM_fp_objects.Filename.apply(lambda x: Utils.convert_tile_img_pth_to_basename(x)).unique()\n",
    "\n",
    "# pred_df = pd.read_csv(r\"D:\\ageglio-1\\gobyfinder_yolov8\\output\\test_runs\\run13-tile-test\\predictions.csv\", index_col=0)\n",
    "# pred_df_filt = pred_df.copy()\n",
    "# pred_df_filt[\"tile_path\"] = get_tiles_paths_from_tile_names(pred_df_filt.Filename, BASE_DIR= r\"D:\\datasets\\tiled\")\n",
    "# assert pred_df_filt.tile_path.notna().all()\n",
    "\n",
    "# print(\"n fp objects\", len(HNM_fp_objects))\n",
    "# print(\"n fp tiles\", len(HNM_fp_tiles))\n",
    "# print(\"n fp full imgs\", len(HNM_fp_full_images))\n",
    "\n",
    "# assert len(pred_df_filt[pred_df_filt.Filename.isin(HNM_fp_tiles)].Filename.unique()) == len(HNM_fp_tiles)\n",
    "\n",
    "# # write the yolo labels and filepath data using the create_conf_batches function\n",
    "# # Bins the dataframe samples into batches of EXACTLY 'batch_size' unique images (except the final batch).\n",
    "# output_dir = r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\"\n",
    "# create_conf_batches(HNM_fp_objects, pred_df_filt, output_dir, batch_size=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad5a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_batch_paths_tile_relabel(all_images_df, all_sampled_HNM_image_lst):\n",
    "    \"\"\"\n",
    "    Exports full and tiled image/label paths for relabeling batches.\n",
    "    Prevents resampling and re-exporting of batches that already have an 'images.txt' list.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Setup ---\n",
    "    BASE_DIR = r\"D:\\datasets\\tiled\"\n",
    "    SPLITS = [\"train\", \"test\", \"validation\"]\n",
    "    all_tiled_image_paths, all_tiled_label_paths = Utils.get_all_img_lbl_pths(BASE_DIR, SPLITS)\n",
    "\n",
    "    BATCH_SIZE = 100\n",
    "    innodata_update = r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\"\n",
    "\n",
    "    # ============================================================\n",
    "    # --- STEP 0: REMOVE ALL IMAGES ALREADY USED IN BATCH 00 + 01\n",
    "    # ============================================================\n",
    "\n",
    "    # Convert tiled image paths → full-image basenames\n",
    "    # Example: Z/.../tile_123_0_0.png → \"123\"\n",
    "    already_used_basenames = {\n",
    "        Utils.convert_tile_img_pth_to_basename(p) for p in all_sampled_HNM_image_lst\n",
    "    }\n",
    "\n",
    "    # Drop them from the pool\n",
    "    used_indices = all_images_df[all_images_df.Filename.isin(already_used_basenames)].index\n",
    "    remaining_images_df = all_images_df.drop(used_indices)\n",
    "\n",
    "    print(f\"Removed {len(used_indices)} images already used in HNM.\")\n",
    "\n",
    "    # ============================================================\n",
    "    # --- STEP 1: Handle the Initial Samples (Batch 00)\n",
    "    # ============================================================\n",
    "\n",
    "    name_initial = \"2025_goby_relabel_batch_00\"\n",
    "    initial_full_dir = os.path.join(innodata_update, name_initial, \"full\")\n",
    "    initial_images_list_path = os.path.join(initial_full_dir, \"images.txt\")\n",
    "\n",
    "    if os.path.exists(initial_images_list_path):\n",
    "        initial_images = Utils.read_list_txt(initial_images_list_path)\n",
    "        initial_bn = {os.path.basename(x).split(\".\")[0] for x in initial_images}\n",
    "\n",
    "        # Remove batch 00 images (safe even if already removed above)\n",
    "        initial_indices = remaining_images_df[remaining_images_df.Filename.isin(initial_bn)].index\n",
    "        remaining_images_df = remaining_images_df.drop(initial_indices)\n",
    "        print(f\"Batch 00: Removed {len(initial_indices)} images from remaining pool.\")\n",
    "    else:\n",
    "        print(f\"Error: Initial sample list not found at {initial_images_list_path}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # ============================================================\n",
    "    # --- STEP 1B: Handle Batch 01 (also already completed)\n",
    "    # ============================================================\n",
    "\n",
    "    name_batch01 = \"2025_goby_relabel_batch_01\"\n",
    "    batch01_full_dir = os.path.join(innodata_update, name_batch01, \"full\")\n",
    "    batch01_images_list_path = os.path.join(batch01_full_dir, \"images.txt\")\n",
    "\n",
    "    if os.path.exists(batch01_images_list_path):\n",
    "        batch01_images = Utils.read_list_txt(batch01_images_list_path)\n",
    "        batch01_bn = {os.path.basename(x).split(\".\")[0] for x in batch01_images}\n",
    "\n",
    "        batch01_indices = remaining_images_df[remaining_images_df.Filename.isin(batch01_bn)].index\n",
    "        remaining_images_df = remaining_images_df.drop(batch01_indices)\n",
    "        print(f\"Batch 01: removed {len(batch01_indices)} images from remaining pool.\")\n",
    "    else:\n",
    "        print(\"WARNING: Batch 01 images.txt not found — skipping explicit removal.\")\n",
    "\n",
    "    # ============================================================\n",
    "    # --- STEP 2: Process Subsequent Batches (Start at Batch 02)\n",
    "    # ============================================================\n",
    "\n",
    "    batch_number = 2   # <-- CRITICAL: start at batch 02\n",
    "    total_new_batches = 0\n",
    "\n",
    "    while len(remaining_images_df) > 0:\n",
    "        name = f\"2025_goby_relabel_batch_{batch_number:02d}\"\n",
    "\n",
    "        tile_img_dir = os.path.join(innodata_update, name, \"tiled\", \"images\")\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # If batch already exists → skip it\n",
    "        # --------------------------------------------------------\n",
    "        if os.path.exists(tile_img_dir):\n",
    "            filenames = os.listdir(tile_img_dir)\n",
    "            tiled_imgs = [f for f in filenames if f.endswith(\".png\") or f.endswith(\".jpg\")]\n",
    "\n",
    "            if len(tiled_imgs) > 0:\n",
    "                existing_tiled_txt = os.path.join(innodata_update, name, \"tiled\", \"images.txt\")\n",
    "                print(f\"Skipping {name}: {len(tiled_imgs)} already exist. Removing images from remaining pool.\")\n",
    "\n",
    "                Utils.check_txt_file_vs_images(existing_tiled_txt, tile_img_dir)\n",
    "\n",
    "                # Convert tiled paths → basenames\n",
    "                existing_tiled_list = Utils.read_list_txt(existing_tiled_txt)\n",
    "                existing_basenames = {\n",
    "                    Utils.convert_tile_img_pth_to_basename(p) for p in existing_tiled_list\n",
    "                }\n",
    "\n",
    "                sampled_indices = remaining_images_df[\n",
    "                    remaining_images_df.Filename.isin(existing_basenames)\n",
    "                ].index\n",
    "\n",
    "                remaining_images_df = remaining_images_df.drop(sampled_indices)\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # Create a new batch\n",
    "        # --------------------------------------------------------\n",
    "        else:\n",
    "            total_new_batches += 1\n",
    "            print(f\"Creating new batch: {name}\")\n",
    "\n",
    "            current_batch_size = min(BATCH_SIZE, len(remaining_images_df))\n",
    "            subset = remaining_images_df.sample(n=current_batch_size, random_state=batch_number)\n",
    "\n",
    "            # Export full images\n",
    "            full_dir = os.path.join(innodata_update, name, \"full\")\n",
    "            os.makedirs(full_dir, exist_ok=True)\n",
    "            Utils.write_list_txt(subset.image_path.values, os.path.join(full_dir, \"images.txt\"))\n",
    "            Utils.write_list_txt(subset.label_path.values, os.path.join(full_dir, \"labels.txt\"))\n",
    "\n",
    "            # Export tiled images\n",
    "            tiled_images, tiled_labels = Utils.list_tiled_set(\n",
    "                subset.Filename.values, all_tiled_image_paths, all_tiled_label_paths\n",
    "            )\n",
    "            tile_dir = os.path.join(innodata_update, name, \"tiled\")\n",
    "            os.makedirs(tile_dir, exist_ok=True)\n",
    "            Utils.write_list_txt(tiled_images, os.path.join(tile_dir, \"images.txt\"))\n",
    "            Utils.write_list_txt(tiled_labels, os.path.join(tile_dir, \"labels.txt\"))\n",
    "\n",
    "            # Remove sampled items\n",
    "            remaining_images_df.drop(subset.index, inplace=True)\n",
    "\n",
    "        batch_number += 1\n",
    "\n",
    "    print(f\"Total batches checked: {batch_number - 1}\")\n",
    "    print(f\"Total NEW batches created: {total_new_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sampled_HNM_image_lst_files = glob.glob(\n",
    "    r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\HNM\\*\\images.txt\"\n",
    ")\n",
    "all_sampled_HNM_image_lst = []\n",
    "for file in all_sampled_HNM_image_lst_files:\n",
    "    all_sampled_HNM_image_lst.extend(Utils.read_list_txt(file))\n",
    "print(\"all HNM tiles list\", len(all_sampled_HNM_image_lst))\n",
    "\n",
    "all_images_df = df_abs2_meta_nf\n",
    "# export_batch_paths_tile_relabel(all_images_df, all_sampled_HNM_image_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568fc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copy Batches\n",
    "completed_batches = [\"00\", \"01\", \"02\", \"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"] # Change this to the desired batch number\n",
    "# for batch in batches:\n",
    "#     path = f\"Z:\\\\__AdvancedTechnologyBackup\\\\04_ProjectData\\\\Innodata_2025\\\\Goby\\\\relabel\\\\2025_goby_relabel_batch_{batch}\\\\tiled\"\n",
    "#     copy_zip_batch_images_labels(path, zip_it=True, gt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_subsampled_transect_tiled_images_labels_with_objects_for_labeling(collect_id, run_copy=True):\n",
    "    # assuming the collect_id inference run is stored locally\n",
    "    all_tiled_image_paths, all_tiled_label_paths = glob.glob(f\"..\\\\output\\\\inference\\\\{collect_id}\\\\tiled\\\\images\\\\*.png\"), glob.glob(f\"..\\\\output\\\\inference\\\\{collect_id}\\\\tiled\\\\labels\\\\*.txt\")\n",
    "    print(len(all_tiled_image_paths), len(all_tiled_label_paths))\n",
    "    assert len(all_tiled_image_paths) == len(all_tiled_label_paths), \"Mismatch between image and label counts.\"\n",
    "    full_imgs = Utils.read_list_txt(f\"..\\\\output\\\\transects\\\\{collect_id}\\\\subsampled_images_0.3\\\\images.txt\")\n",
    "    tiled_images, tiled_labels  = Utils.list_tiled_set(full_imgs, all_tiled_image_paths, all_tiled_label_paths)\n",
    "    assert len(tiled_images) > 0, \"No tiled images found\"\n",
    "    assert len(tiled_images) == len(tiled_labels), \"Mismatch between number of images and labels\"\n",
    "    print(f\"Number of tiled images from full_imgs: {len(tiled_images)}\")\n",
    "    pd.Series(tiled_images).to_csv(f\"..\\\\output\\\\inference\\\\{collect_id}\\\\tiled\\\\selected_tiled_images.csv\", header=False, index=False)\n",
    "    pd.Series(tiled_labels).to_csv(f\"..\\\\output\\\\inference\\\\{collect_id}\\\\tiled\\\\selected_tiled_labels.csv\", header=False, index=False)\n",
    "    # Write the tiled labels to the dataset folder if they are not empty\n",
    "    innodata_project_folder = f\"Z:\\\\__AdvancedTechnologyBackup\\\\04_ProjectData\\\\Innodata_2025\\\\Goby\\\\transects\\\\{collect_id}\"\n",
    "    img_folder = f\"{innodata_project_folder}\\\\images\"\n",
    "    lbl_folder = f\"{innodata_project_folder}\\\\labels\"\n",
    "    if not os.path.exists(img_folder):\n",
    "        os.makedirs(img_folder)\n",
    "    if not os.path.exists(lbl_folder):\n",
    "        os.makedirs(lbl_folder)\n",
    "    for img, lbl in tqdm(zip(tiled_images, tiled_labels)):\n",
    "        if os.path.exists(lbl) and os.path.getsize(lbl) > 0:\n",
    "            shutil.copy2(lbl, lbl_folder)\n",
    "            shutil.copy2(img, img_folder)\n",
    "# tranect tiling and preping for labeling\n",
    "collect_id = \"20240618_001_Iver3069_ABS2\"\n",
    "# collect_id = \"20240804_001_Iver3069_ABS2\"\n",
    "# copy_subsampled_transect_tiled_images_labels_with_objects_for_labeling(collect_id, run_copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739603b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_batch_paths_tile_transect(collect_id):\n",
    "    \"\"\"\n",
    "    Exports tiled image/label paths for transect batches.\n",
    "    Prevents resampling of transects that already have an export directory.\n",
    "    Samples by full image basename to ensure all tiles for a full image stay together.\n",
    "    \"\"\"\n",
    "    # Define constants\n",
    "    BATCH_SIZE = 100\n",
    "    transect_update = f\"Z:\\\\__AdvancedTechnologyBackup\\\\04_ProjectData\\\\Innodata_2025\\\\Goby\\\\transects\\\\{collect_id}\"\n",
    "\n",
    "    # Step 1: Create a DataFrame with ALL existing TILE paths\n",
    "    # Note: Use os.path.join for robust path construction in glob\n",
    "    base_dir = os.path.join(transect_update, \"images\")\n",
    "    all_image_tiles = glob.glob(os.path.join(base_dir, \"*.png\"))\n",
    "    all_label_tiles = glob.glob(os.path.join(transect_update, \"labels\", \"*.txt\"))\n",
    "    \n",
    "    # Check for empty data before proceeding\n",
    "    if not all_image_tiles or not all_label_tiles:\n",
    "        print(f\"No tiled data found for {collect_id}. Exiting.\")\n",
    "        return 0\n",
    "\n",
    "    df = pd.DataFrame(np.c_[all_image_tiles, all_label_tiles], columns=[\"image_path\", \"label_path\"])\n",
    "    \n",
    "    # CRITICAL: Extract the 'Full Image Basename' from the tile label path\n",
    "    # Assuming tile paths look like 'full_image_name_x000_y000.txt'\n",
    "    df['basename'] = df.label_path.apply(lambda x: os.path.basename(x).rsplit('_', 2)[0])\n",
    "    \n",
    "    # The pool of images to sample from is the UNIQUE full image basenames\n",
    "    # Use tolist() and convert back to Series later for easier indexing/sampling\n",
    "    unbatched_basenames_list = df['basename'].unique().tolist()\n",
    "    unbatched_basenames = pd.Series(unbatched_basenames_list)\n",
    "\n",
    "    # Now, we proceed with the batching by basename\n",
    "    batch_number = 0\n",
    "    total_new_batches = 0\n",
    "    total_tiles = 0\n",
    "    \n",
    "    while len(unbatched_basenames) > 0:\n",
    "        name = f\"{collect_id}_batch_{batch_number:02d}\"\n",
    "        transect_path = os.path.join(transect_update, name)\n",
    "        \n",
    "        # --- Skipping Logic ---\n",
    "        images_txt_path = os.path.join(transect_path, \"images.txt\")\n",
    "        if os.path.exists(images_txt_path):\n",
    "            print(f\"Skipping existing batch: {name}\")\n",
    "            \n",
    "            # Read the list of TILED images that were exported\n",
    "            # Assumes Utils.read_list_txt reads the contents of the file\n",
    "            exported_tiled_paths = Utils.read_list_txt(images_txt_path)\n",
    "            total_tiles += len(exported_tiled_paths)\n",
    "            print(f\"Number of tiled images in existing batch: {len(exported_tiled_paths)}\")\n",
    "            # Convert the tiled paths back to their full image basenames\n",
    "            # Assuming Utils.convert_tile_img_pth_to_basename extracts the full image basename\n",
    "            # e.g., converts 'path/to/img_x000_y000.png' to 'img'\n",
    "            exported_basenames = {Utils.convert_tile_img_pth_to_basename(x) for x in exported_tiled_paths}\n",
    "            print(f\"Number of unique basenames in existing batch: {len(exported_basenames)}\")\n",
    "            # Filter the unbatched pool to remove the basenames that are already in this batch\n",
    "            # Convert Series to list/set for difference, then back to Series for sampling\n",
    "            remaining_basenames_set = set(unbatched_basenames.tolist()).difference(exported_basenames)\n",
    "            unbatched_basenames = pd.Series(list(remaining_basenames_set)) # Restore as Series\n",
    "            \n",
    "            batch_number += 1\n",
    "            continue # Go to the next loop iteration (next batch number)\n",
    "            \n",
    "        # --- Create a New Batch ---\n",
    "        else:\n",
    "            total_new_batches += 1\n",
    "            print(f\"Creating new batch: {name}\")\n",
    "            \n",
    "            current_batch_size = min(BATCH_SIZE, len(unbatched_basenames))\n",
    "\n",
    "            # Sample the unique full image basenames from the remaining pool\n",
    "            # Use random_state=batch_number to make the sample repeatable for this batch number\n",
    "            # but only if you want repeatability. The prompt said \"Do NOT set random_state\" \n",
    "            # for different random samples, so we'll remove it.\n",
    "            # Fix: Since unbatched_basenames is a Series of the BASENAMES, sampling works\n",
    "            # Sample by position (frac=None) since index might be arbitrary\n",
    "            subset_basenames = unbatched_basenames.sample(\n",
    "                n=current_batch_size, random_state=None, replace=False\n",
    "            ) \n",
    "            print(f\"Sampled {len(subset_basenames)} basenames for new batch.\")\n",
    "            # Get ALL tile paths associated with the sampled basenames\n",
    "            subset_df = df[df['basename'].isin(subset_basenames.tolist())]\n",
    "            print(f\"Number of TILE images/labels in new batch: {len(subset_df)}\")\n",
    "            total_tiles += len(subset_df)\n",
    "            # Export the list of TILE paths\n",
    "            os.makedirs(transect_path, exist_ok=True)\n",
    "            Utils.write_list_txt(subset_df.image_path.tolist(), os.path.join(transect_path, \"images.txt\"))\n",
    "            Utils.write_list_txt(subset_df.label_path.tolist(), os.path.join(transect_path, \"labels.txt\"))\n",
    "            \n",
    "            # CRITICAL: Remove the sampled basenames from the remaining pool for the next iteration\n",
    "            remaining_basenames_set = set(unbatched_basenames.tolist()).difference(set(subset_basenames.tolist()))\n",
    "            unbatched_basenames = pd.Series(list(remaining_basenames_set))\n",
    "            \n",
    "            # Increment the batch counter\n",
    "            batch_number += 1\n",
    "\n",
    "    print(f\"Total batches checked: {batch_number}\")\n",
    "    print(f\"Total NEW batches created: {total_new_batches}\")\n",
    "    print(f\"Total TILE images processed: {total_tiles}\")\n",
    "    return batch_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transect batches\n",
    "collect_id = \"20240804_001_Iver3069_ABS2\"\n",
    "# collect_id = \"20240618_001_Iver3069_ABS2\"\n",
    "n_batches = export_batch_paths_tile_transect(collect_id = collect_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8106ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_transect_batch_image_labels(collect_id, batch = 0, copy=False):  # Change this to the desired batch number\n",
    "    # # Copy Batches\n",
    "    transect_update = f\"Z:\\\\__AdvancedTechnologyBackup\\\\04_ProjectData\\\\Innodata_2025\\\\Goby\\\\transects\\\\{collect_id}\"\n",
    "    path = transect_update + \"\\\\\" + f\"{collect_id}_batch_{batch:02d}\"\n",
    "    images, labels = Utils.read_list_txt(os.path.join(path, \"images.txt\")), Utils.read_list_txt(os.path.join(path, \"labels.txt\"))\n",
    "    img_folder, lbl_folder = os.path.join(path, \"images\"), os.path.join(path, \"labels\")\n",
    "    if copy:\n",
    "        Utils.copy_files_lst(images, img_folder)\n",
    "        Utils.copy_files_lst(labels, lbl_folder)\n",
    "    Utils.check_txt_file_vs_images(os.path.join(path, \"images.txt\"), img_folder)\n",
    "\n",
    "for b in [8]:\n",
    "    copy_transect_batch_image_labels(collect_id=\"20240618_001_Iver3069_ABS2\", batch = b, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6cff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Path Construction ---\n",
    "def get_im_lb_folders(batch, collect_id):\n",
    "    transect_update = f\"Z:\\\\__AdvancedTechnologyBackup\\\\04_ProjectData\\\\Innodata_2025\\\\Goby\\\\transects\\\\{collect_id}\"\n",
    "    batch_name = f\"{collect_id}_batch_{batch:02d}\"\n",
    "    path = os.path.join(transect_update, batch_name)\n",
    "    img_folder = os.path.join(path, \"images\")\n",
    "    lbl_folder = os.path.join(path, \"yolo_labels\")\n",
    "    return img_folder, lbl_folder\n",
    "\n",
    "\n",
    "# batches = [8]\n",
    "# collect_id = \"20240618_001_Iver3069_ABS2\"\n",
    "# for batch in batches:\n",
    "#     img_folder, lbl_folder = get_im_lb_folders(batch, collect_id)\n",
    "#     Utils.zip_folder(img_folder)\n",
    "#     Utils.zip_folder(lbl_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_path_0n = r\"..\\output\\validation\\detect\\test_sept_1fish\\test_run13_1n_curves.csv\"\n",
    "support_0n = df_abs2_meta_nf[(df_abs2_meta_nf.n_fish < 2)].shape[0]\n",
    "curve_path_2n = r\"..\\output\\validation\\detect\\test_sept_2-3fish\\test_run13_2-3n_curves.csv\"\n",
    "support_2n = df_abs2_meta_nf[(df_abs2_meta_nf.n_fish >= 2) & (df_abs2_meta_nf.n_fish <= 3)].shape[0]\n",
    "curve_path_4n = r\"..\\output\\validation\\detect\\test_sept_4fish+\\test_run13_4n+_curves.csv\"\n",
    "support_4n = df_abs2_meta_nf[(df_abs2_meta_nf.n_fish >= 4)].shape[0]\n",
    "curve_path_abs2 = r\"..\\output\\validation\\detect\\test_sept_abiss2\\test_run13_abiss2_curves.csv\"\n",
    "support_abs2 = df_abs2_meta_nf.shape[0]\n",
    "curve_path_abs1 = r\"..\\output\\validation\\detect\\test_sept_abiss1\\test_run13_abiss1_curves.csv\"\n",
    "support_abs1 = df_abs1_meta_nf.shape[0]\n",
    "df_0n, fmax_0n, cmax_0n, c_eq_0n, pr_eq_0n = Reports.get_metrics(curve_path_0n) \n",
    "df_2n, fmax_2n, cmax_2n, c_eq_2n, pr_eq_2n = Reports.get_metrics(curve_path_2n) \n",
    "df_4n, fmax_4n, cmax_4n, c_eq_4n, pr_eq_4n = Reports.get_metrics(curve_path_4n) \n",
    "dfabs2, fmaxabs2, cmaxabs2, c_eqabs2, pr_eqabs2 = Reports.get_metrics(curve_path_abs2) \n",
    "dfabs1, fmaxabs1, cmaxabs1, c_eqabs1, pr_eqabs1 = Reports.get_metrics(curve_path_abs1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_0n.precision, df_0n.recall, label=f\"ABISS2 only 1 fish, {support_0n}, {support_0n/support_abs2*100:0.1f}%\", color=\"purple\")\n",
    "plt.plot(df_2n.precision, df_2n.recall, label=f\"ABISS2 At 2-3 fish, {support_2n}, {support_2n/support_abs2*100:0.1f}%\", color=\"green\")\n",
    "plt.plot(df_4n.precision, df_4n.recall, label=f\"ABISS2 At least 4 fish, {support_4n}, {support_4n/support_abs2*100:0.1f}%\", color=\"orange\")\n",
    "plt.plot(dfabs1.precision, dfabs1.recall, label=f\"ABISS1 Test images, {support_abs1}\", color=\"red\")\n",
    "plt.xlabel('Recall') \n",
    "plt.ylabel('Precision') \n",
    "plt.title('Goby count bins Precision-Recall Curve')\n",
    "plt.legend() # 1461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8decc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dfabs2.precision, dfabs2.recall, label=f\"ABISS2 Test images, {support_abs2}\")\n",
    "plt.plot(dfabs1.precision, dfabs1.recall, label=f\"ABISS1 Test images, {support_abs1}\", color=\"red\")\n",
    "plt.xlabel('Recall') \n",
    "plt.ylabel('Precision') \n",
    "plt.title('ABISS1 vs ABISS2 Precision-Recall Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob(r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\\2025_goby_relabel_initial_sample\\full\\Innodata Output\\Initial Re-Label Full\\json\\*.json\")\n",
    "image_lst = glob.glob(r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\\2025_goby_relabel_initial_sample\\full\\original\\images\\images\\*.png\")\n",
    "image_path = image_lst[2]\n",
    "json_path = json_files[2]\n",
    "Overlays.plot_coco_boxes(image_path, json_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolov8v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
