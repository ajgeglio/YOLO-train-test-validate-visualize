{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746fb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"src\")))\n",
    "from utils import Utils\n",
    "from reports import Reports\n",
    "import matplotlib.pyplot as plt\n",
    "from overlays import Overlays\n",
    "from utils import Utils\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a48d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for images with n_fish >= n images\n",
    "# df_run12_meta_nf = pd.read_csv(r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\runs\\run12\\run12_metadata.csv\", index_col=0, low_memory=False)\n",
    "all_meta_nf = pd.read_csv(r\"Z:\\__AdvancedTechnologyBackup\\07_Database\\MetadataCombined\\all_annotated_meta_splits_filtered_20251030.csv\", index_col=0, low_memory=False)\n",
    "df_abs2_meta_nf = all_meta_nf[(all_meta_nf.n_fish >= 0)  & (all_meta_nf.imh > 2176)]\n",
    "df_abs1_meta_nf = all_meta_nf[(all_meta_nf.n_fish >= 0)  & (all_meta_nf.imh == 2176)]\n",
    "abiss2_len = len(df_abs2_meta_nf) \n",
    "abiss1_len = len(df_abs1_meta_nf)  \n",
    "# export_paths(df_abs1_meta_nf[df_abs1_meta_nf.split == \"test\"], run13_folder, subfolder=\"abiss1_test\")\n",
    "print(\"usable Abiss1 images:\", abiss1_len)\n",
    "print(\"usable Abiss2 images:\", abiss2_len) #usable Abiss2 images: 14225\n",
    "few_fish_df = df_abs2_meta_nf[(df_abs2_meta_nf.n_fish < 2)]\n",
    "new_path = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\one_fish\\images\"\n",
    "output_dir = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\one_fish\\overlays\"\n",
    "# Overlays.plot_label_overlays(few_fish_df.image_path, few_fish_df.label_path, output_dir, overwrite=False)\n",
    "# usable Abiss1 images: 33356\n",
    "# usable Abiss2 images: 13549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles_paths_from_batch_dir(BATCH_DIR, BASE_DIR, SPLITS):\n",
    "    all_tiled_image_paths, all_tiled_label_paths = Utils.get_all_tiled_img_lbl_pths(BASE_DIR, SPLITS)\n",
    "    batch_tiled_im = os.listdir(BATCH_DIR+\"\\\\\"+\"images\")\n",
    "    batch_tiled_lb = os.listdir(BATCH_DIR+\"\\\\\"+\"labels\")\n",
    "    batch_tiled_im_no_ext = list(map(lambda x: x.split(\".\")[0], batch_tiled_im))\n",
    "    batch_tiled_lb_no_ext = list(map(lambda x: x.split(\".\")[0], batch_tiled_lb))\n",
    "\n",
    "    # --- Refactored & Completed Assignments ---\n",
    "\n",
    "    # 1. Create basename-to-fullpath dictionaries (The key step for efficient lookup)\n",
    "    # Note: zip is often cleaner than map(lambda...) for pairing elements.\n",
    "    img_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_tiled_image_paths}\n",
    "    lbl_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_tiled_label_paths}\n",
    "\n",
    "    # 2. Use list comprehensions to look up the full paths using the batch basenames\n",
    "    # This is much faster than searching a list repeatedly.\n",
    "\n",
    "    batch_tiled_im_fp = [img_path_map_no_ext[bn] for bn in batch_tiled_im_no_ext if bn in img_path_map_no_ext]\n",
    "    batch_tiled_lb_fp = [lbl_path_map_no_ext[bn] for bn in batch_tiled_lb_no_ext if bn in lbl_path_map_no_ext]\n",
    "\n",
    "    # --- Verification (Optional) ---\n",
    "    print(f\"Total full paths found for images in batch: {len(batch_tiled_im_fp)}\")\n",
    "    print(f\"Total full paths found for labels in batch: {len(batch_tiled_lb_fp)}\")\n",
    "    # assert len(batch_tiled_im_fp) == len(batch_tiled_im), \"Some image files not found in the master list.\"\n",
    "    # assert len(batch_tiled_lb_fp) == len(batch_tiled_lb), \"Some label files not found in the master list.\"\n",
    "    return sorted(batch_tiled_im_fp), sorted(batch_tiled_lb_fp)\n",
    "\n",
    "# BATCH_DIR = r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\transects\\20240804_001_Iver3069_ABS2\\20240804_001_Iver3069_ABS2_batch_00\"\n",
    "# BASE_DIR = r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\transects\\20240804_001_Iver3069_ABS2\"\n",
    "# batch_tiled_im_fp, batch_tiled_lb_fp = get_tiles_paths_from_batch_dir(BATCH_DIR = BATCH_DIR, BASE_DIR = BASE_DIR, SPLITS = None)\n",
    "\n",
    "# Utils.write_list_txt(sorted(batch_tiled_im_fp), BATCH_DIR+\"\\\\\"+\"images.txt\")\n",
    "# Utils.write_list_txt(sorted(batch_tiled_lb_fp), BATCH_DIR+\"\\\\\"+\"labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6154d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_paths_from_tiled_images(tiled_images):\n",
    "    batch_set = set(map(lambda x: Utils.convert_tile_img_pth_to_basename(x), tiled_images))\n",
    "    all_full_imgs = glob.glob(r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\full\\images\\*.png\")\n",
    "    all_full_lbls = glob.glob(r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\full\\labels\\*.txt\")\n",
    "    print(\"full images\", len(all_full_lbls))\n",
    "    assert len(all_full_imgs) == len(all_full_lbls)\n",
    "    img_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_full_imgs}\n",
    "    lbl_path_map_no_ext = {os.path.basename(p).split(\".\")[0]: p for p in all_full_lbls}\n",
    "    batch_full_im_fp = [img_path_map_no_ext[bn] for bn in batch_set if bn in img_path_map_no_ext]\n",
    "    batch_full_lb_fp = [lbl_path_map_no_ext[bn] for bn in batch_set if bn in lbl_path_map_no_ext]\n",
    "    assert len(batch_full_im_fp) == len(batch_full_lb_fp), \"number of images and labels do not match\"\n",
    "    print(\"full image paths from tiled paths\", len(batch_full_lb_fp))\n",
    "    return batch_full_im_fp, batch_full_lb_fp\n",
    "    \n",
    "# batch_full_im_fp, batch_full_lb_fp = get_full_paths_from_tiled_images(batch_tiled_im_fp)   \n",
    "# Utils.write_list_txt(batch_full_im_fp, r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\\2025_goby_relabel_batch_01\\full\\images.txt\")\n",
    "# Utils.write_list_txt(batch_full_lb_fp, r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\\2025_goby_relabel_batch_01\\full\\labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_batch_paths_tile_relabel(all_images_df):\n",
    "    \"\"\"\n",
    "    Exports full and tiled image/label paths for relabeling batches.\n",
    "    Prevents resampling and re-exporting of batches that already have an 'images.txt' list.\n",
    "    \"\"\"\n",
    "    # --- Setup ---\n",
    "    all_tiled_image_paths, all_tiled_label_paths = Utils.get_all_tiled_img_lbl_pths()\n",
    "    \n",
    "    # Define constants\n",
    "    BATCH_SIZE = 100\n",
    "    innodata_update = r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\"\n",
    "    \n",
    "    # --- Step 1: Handle the Initial Sample (Pre-existing/Fixed List) ---\n",
    "    name_initial = \"2025_goby_relabel_initial_sample\"\n",
    "    initial_full_dir = os.path.join(innodata_update, name_initial, \"full\")\n",
    "    initial_images_list_path = os.path.join(initial_full_dir, \"images.txt\")\n",
    "\n",
    "    try:\n",
    "        initial_images = Utils.read_list_txt(initial_images_list_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Initial sample list not found at {initial_images_list_path}. Exiting.\")\n",
    "        return # Exit if the baseline list is missing\n",
    "\n",
    "    # Get basenames of initial sample images\n",
    "    initial_bn = {os.path.basename(x).split(\".\")[0] for x in initial_images}\n",
    "    \n",
    "    # Remove initial sample images from the pool\n",
    "    initial_indices = all_images_df[all_images_df.Filename.isin(initial_bn)].index\n",
    "    remaining_images_df = all_images_df.drop(initial_indices)\n",
    "\n",
    "    # --- Step 2: Process Subsequent Batches (Repeatable Random Samples) ---\n",
    "    batch_number = 1\n",
    "    total_new_batches = 0\n",
    "    \n",
    "    while len(remaining_images_df) > 0:\n",
    "        name = f\"2025_goby_relabel_batch_{batch_number:02d}\"\n",
    "        \n",
    "        # Define paths for the current batch's tiled output\n",
    "        tile_img_dir = os.path.join(innodata_update, name, \"tiled\", \"images\")\n",
    "        if os.path.exists(tile_img_dir):\n",
    "            filenames = os.listdir(tile_img_dir)\n",
    "            tiled_imgs = [f for f in filenames if f.endswith(\".png\") or f.endswith(\".jpg\")]\n",
    "            # Check if the list for this batch already exists (AVOIDS RESAMPLING)\n",
    "            if len(tiled_imgs) > 0:\n",
    "                existing_tiled_txt = os.path.join(innodata_update, name, \"tiled\", \"images.txt\")\n",
    "                print(f\"Skipping {name}: {len(tiled_imgs)} already exist. Removing images from remaining pool.\")\n",
    "                # integrity check\n",
    "                Utils.check_txt_file_vs_images(existing_tiled_txt, tile_img_dir)\n",
    "                # remove its contents from remaining_images_df\n",
    "                # Find the corresponding full image basenames to drop from the DataFrame\n",
    "                existing_basenames = list(map(lambda x: Utils.convert_tile_img_pth_to_basename(x), existing_tiled_txt))\n",
    "\n",
    "                # --- Robust Dropping Logic ---\n",
    "                # We must use the *full image* names that were sampled to create this batch.\n",
    "                \n",
    "                try:\n",
    "                    sampled_indices = remaining_images_df[\n",
    "                        remaining_images_df.Filename.isin(existing_basenames)\n",
    "                    ].index\n",
    "                    \n",
    "                    remaining_images_df.drop(sampled_indices, inplace=True)\n",
    "                    \n",
    "                except FileNotFoundError:\n",
    "                    print(f\"WARNING: Full list for {name} not found. Could not skip batch.\")\n",
    "                    break # Stop if we can't reliably skip the batch\n",
    "                # ---------------------------\n",
    "            \n",
    "        else:\n",
    "            # --- Create a New Batch ---\n",
    "            total_new_batches += 1\n",
    "            print(f\"Creating new batch: {name}\")\n",
    "            \n",
    "            current_batch_size = min(BATCH_SIZE, len(remaining_images_df))\n",
    "            \n",
    "            # Use random_state based on batch number to make the sample repeatable across runs\n",
    "            # Since the user asked it NOT to change, we must use random_state.\n",
    "            subset = remaining_images_df.sample(n=current_batch_size, random_state=batch_number)\n",
    "            \n",
    "            # Define output paths\n",
    "            full_dir = os.path.join(innodata_update, name, \"full\")\n",
    "            os.makedirs(full_dir, exist_ok=True)\n",
    "            # Export the current sample batch (Full Images)\n",
    "            Utils.write_list_txt(subset.image_path.values , os.path.join(full_dir, \"images.txt\"))\n",
    "            Utils.write_list_txt(subset.label_path.values , os.path.join(full_dir, \"labels.txt\"))\n",
    "            \n",
    "            # List and Export the Tiled Images for this subset\n",
    "            tiled_images, tiled_labels = Utils.list_tiled_set(\n",
    "                subset.Filename.values, all_tiled_image_paths, all_tiled_label_paths\n",
    "            )\n",
    "            tile_dir = os.path.join(innodata_update, name, \"tiled\")\n",
    "            os.makedirs(tile_dir, exist_ok=True)\n",
    "            Utils.write_list_txt(tiled_images , os.path.join(tile_dir, \"images.txt\"))\n",
    "            Utils.write_list_txt(tiled_labels , os.path.join(tile_dir, \"labels.txt\"))\n",
    "            # CRITICAL: Remove the sampled subset from the remaining pool\n",
    "            remaining_images_df.drop(subset.index, inplace=True)\n",
    "            \n",
    "        # Increment the batch counter\n",
    "        batch_number += 1\n",
    "\n",
    "    print(f\"Total batches checked: {batch_number - 1}\")\n",
    "    print(f\"Total NEW batches created: {total_new_batches}\")\n",
    "\n",
    "# all_images_df = df_abs2_meta_nf[(df_abs2_meta_nf.n_fish >= 2)].copy()\n",
    "# all_images_df.shape\n",
    "# export_batch_paths_tile_relabel(all_images_df)\n",
    "\n",
    "\n",
    "# # Copy Batches\n",
    "# def copy_tiled_relabel_batch(batch = \"batch_01\"): # Change this to the desired batch number\n",
    "#     path = f\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\innodata_updates\\\\2025_goby_relabel_{batch}\\\\tiled\"\n",
    "#     images, labels = Utils.read_list_txt(os.path.join(path, \"images.txt\")), Utils.read_list_txt(os.path.join(path, \"labels.txt\"))\n",
    "#     img_folder, lbl_folder = os.path.join(path, \"images\"), os.path.join(path, \"labels\")\n",
    "#     Utils.copy_files_lst(images, img_folder)\n",
    "#     Utils.copy_files_lst(labels, lbl_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_subsampled_transect_tiled_images_labels_with_objects_for_labeling(collect_id, run_copy=True):\n",
    "    # assuming the collect_id inference run is stored locally\n",
    "    all_tiled_image_paths, all_tiled_label_paths = glob.glob(f\"..\\\\output\\\\inference\\\\{collect_id}\\\\tiled\\\\images\\\\*.png\"), glob.glob(f\"..\\\\output\\\\inference\\\\{collect_id}\\\\tiled\\\\labels\\\\*.txt\")\n",
    "    print(len(all_tiled_image_paths), len(all_tiled_label_paths))\n",
    "    assert len(all_tiled_image_paths) == len(all_tiled_label_paths), \"Mismatch between image and label counts.\"\n",
    "    full_imgs = Utils.read_list_txt(f\"..\\\\output\\\\transects\\\\{collect_id}\\\\subsampled_images_0.3\\\\images.txt\")\n",
    "    tiled_images, tiled_labels  = Utils.list_tiled_set(full_imgs, all_tiled_image_paths, all_tiled_label_paths)\n",
    "    assert len(tiled_images) > 0, \"No tiled images found\"\n",
    "    assert len(tiled_images) == len(tiled_labels), \"Mismatch between number of images and labels\"\n",
    "    print(f\"Number of tiled images from full_imgs: {len(tiled_images)}\")\n",
    "    pd.Series(tiled_images).to_csv(f\"..\\\\output\\\\inference\\\\{collect_id}\\\\tiled\\\\selected_tiled_images.csv\", header=False, index=False)\n",
    "    pd.Series(tiled_labels).to_csv(f\"..\\\\output\\\\inference\\\\{collect_id}\\\\tiled\\\\selected_tiled_labels.csv\", header=False, index=False)\n",
    "    # Write the tiled labels to the dataset folder if they are not empty\n",
    "    innodata_project_folder = f\"Z:\\\\__AdvancedTechnologyBackup\\\\04_ProjectData\\\\Innodata_2025\\\\Goby\\\\transects\\\\{collect_id}\"\n",
    "    img_folder = f\"{innodata_project_folder}\\\\images\"\n",
    "    lbl_folder = f\"{innodata_project_folder}\\\\labels\"\n",
    "    if not os.path.exists(img_folder):\n",
    "        os.makedirs(img_folder)\n",
    "    if not os.path.exists(lbl_folder):\n",
    "        os.makedirs(lbl_folder)\n",
    "    for img, lbl in tqdm(zip(tiled_images, tiled_labels)):\n",
    "        if os.path.exists(lbl) and os.path.getsize(lbl) > 0:\n",
    "            shutil.copy2(lbl, lbl_folder)\n",
    "            shutil.copy2(img, img_folder)\n",
    "# tranect tiling and preping for labeling\n",
    "collect_id = \"20240618_001_Iver3069_ABS2\"\n",
    "# collect_id = \"20240804_001_Iver3069_ABS2\"\n",
    "# copy_subsampled_transect_tiled_images_labels_with_objects_for_labeling(collect_id, run_copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739603b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_batch_paths_tile_transect(collect_id):\n",
    "    \"\"\"\n",
    "    Exports tiled image/label paths for transect batches.\n",
    "    Prevents resampling of transects that already have an export directory.\n",
    "    Samples by full image basename to ensure all tiles for a full image stay together.\n",
    "    \"\"\"\n",
    "    # Define constants\n",
    "    BATCH_SIZE = 100\n",
    "    transect_update = f\"Z:\\\\__AdvancedTechnologyBackup\\\\04_ProjectData\\\\Innodata_2025\\\\Goby\\\\transects\\\\{collect_id}\"\n",
    "\n",
    "    # Step 1: Create a DataFrame with ALL existing TILE paths\n",
    "    # Note: Use os.path.join for robust path construction in glob\n",
    "    base_dir = os.path.join(transect_update, \"images\")\n",
    "    all_image_tiles = glob.glob(os.path.join(base_dir, \"*.png\"))\n",
    "    all_label_tiles = glob.glob(os.path.join(transect_update, \"labels\", \"*.txt\"))\n",
    "    \n",
    "    # Check for empty data before proceeding\n",
    "    if not all_image_tiles or not all_label_tiles:\n",
    "        print(f\"No tiled data found for {collect_id}. Exiting.\")\n",
    "        return 0\n",
    "\n",
    "    df = pd.DataFrame(np.c_[all_image_tiles, all_label_tiles], columns=[\"image_path\", \"label_path\"])\n",
    "    \n",
    "    # CRITICAL: Extract the 'Full Image Basename' from the tile label path\n",
    "    # Assuming tile paths look like 'full_image_name_x000_y000.txt'\n",
    "    df['basename'] = df.label_path.apply(lambda x: os.path.basename(x).rsplit('_', 2)[0])\n",
    "    \n",
    "    # The pool of images to sample from is the UNIQUE full image basenames\n",
    "    # Use tolist() and convert back to Series later for easier indexing/sampling\n",
    "    unbatched_basenames_list = df['basename'].unique().tolist()\n",
    "    unbatched_basenames = pd.Series(unbatched_basenames_list)\n",
    "\n",
    "    # Now, we proceed with the batching by basename\n",
    "    batch_number = 0\n",
    "    total_new_batches = 0\n",
    "    total_tiles = 0\n",
    "    \n",
    "    while len(unbatched_basenames) > 0:\n",
    "        name = f\"{collect_id}_batch_{batch_number:02d}\"\n",
    "        transect_path = os.path.join(transect_update, name)\n",
    "        \n",
    "        # --- Skipping Logic ---\n",
    "        images_txt_path = os.path.join(transect_path, \"images.txt\")\n",
    "        if os.path.exists(images_txt_path):\n",
    "            print(f\"Skipping existing batch: {name}\")\n",
    "            \n",
    "            # Read the list of TILED images that were exported\n",
    "            # Assumes Utils.read_list_txt reads the contents of the file\n",
    "            exported_tiled_paths = Utils.read_list_txt(images_txt_path)\n",
    "            total_tiles += len(exported_tiled_paths)\n",
    "            print(f\"Number of tiled images in existing batch: {len(exported_tiled_paths)}\")\n",
    "            # Convert the tiled paths back to their full image basenames\n",
    "            # Assuming Utils.convert_tile_img_pth_to_basename extracts the full image basename\n",
    "            # e.g., converts 'path/to/img_x000_y000.png' to 'img'\n",
    "            exported_basenames = {Utils.convert_tile_img_pth_to_basename(x) for x in exported_tiled_paths}\n",
    "            print(f\"Number of unique basenames in existing batch: {len(exported_basenames)}\")\n",
    "            # Filter the unbatched pool to remove the basenames that are already in this batch\n",
    "            # Convert Series to list/set for difference, then back to Series for sampling\n",
    "            remaining_basenames_set = set(unbatched_basenames.tolist()).difference(exported_basenames)\n",
    "            unbatched_basenames = pd.Series(list(remaining_basenames_set)) # Restore as Series\n",
    "            \n",
    "            batch_number += 1\n",
    "            continue # Go to the next loop iteration (next batch number)\n",
    "            \n",
    "        # --- Create a New Batch ---\n",
    "        else:\n",
    "            total_new_batches += 1\n",
    "            print(f\"Creating new batch: {name}\")\n",
    "            \n",
    "            current_batch_size = min(BATCH_SIZE, len(unbatched_basenames))\n",
    "\n",
    "            # Sample the unique full image basenames from the remaining pool\n",
    "            # Use random_state=batch_number to make the sample repeatable for this batch number\n",
    "            # but only if you want repeatability. The prompt said \"Do NOT set random_state\" \n",
    "            # for different random samples, so we'll remove it.\n",
    "            # Fix: Since unbatched_basenames is a Series of the BASENAMES, sampling works\n",
    "            # Sample by position (frac=None) since index might be arbitrary\n",
    "            subset_basenames = unbatched_basenames.sample(\n",
    "                n=current_batch_size, random_state=None, replace=False\n",
    "            ) \n",
    "            print(f\"Sampled {len(subset_basenames)} basenames for new batch.\")\n",
    "            # Get ALL tile paths associated with the sampled basenames\n",
    "            subset_df = df[df['basename'].isin(subset_basenames.tolist())]\n",
    "            print(f\"Number of TILE images/labels in new batch: {len(subset_df)}\")\n",
    "            total_tiles += len(subset_df)\n",
    "            # Export the list of TILE paths\n",
    "            os.makedirs(transect_path, exist_ok=True)\n",
    "            Utils.write_list_txt(subset_df.image_path.tolist(), os.path.join(transect_path, \"images.txt\"))\n",
    "            Utils.write_list_txt(subset_df.label_path.tolist(), os.path.join(transect_path, \"labels.txt\"))\n",
    "            \n",
    "            # CRITICAL: Remove the sampled basenames from the remaining pool for the next iteration\n",
    "            remaining_basenames_set = set(unbatched_basenames.tolist()).difference(set(subset_basenames.tolist()))\n",
    "            unbatched_basenames = pd.Series(list(remaining_basenames_set))\n",
    "            \n",
    "            # Increment the batch counter\n",
    "            batch_number += 1\n",
    "\n",
    "    print(f\"Total batches checked: {batch_number}\")\n",
    "    print(f\"Total NEW batches created: {total_new_batches}\")\n",
    "    print(f\"Total TILE images processed: {total_tiles}\")\n",
    "    return batch_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transect batches\n",
    "collect_id = \"20240804_001_Iver3069_ABS2\"\n",
    "# collect_id = \"20240618_001_Iver3069_ABS2\"\n",
    "n_batches = export_batch_paths_tile_transect(collect_id = collect_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8106ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_transect_batch_image_labels(collect_id, batch = 0, copy=False):  # Change this to the desired batch number\n",
    "    # # Copy Batches\n",
    "    transect_update = f\"Z:\\\\__AdvancedTechnologyBackup\\\\04_ProjectData\\\\Innodata_2025\\\\Goby\\\\transects\\\\{collect_id}\"\n",
    "    path = transect_update + \"\\\\\" + f\"{collect_id}_batch_{batch:02d}\"\n",
    "    images, labels = Utils.read_list_txt(os.path.join(path, \"images.txt\")), Utils.read_list_txt(os.path.join(path, \"labels.txt\"))\n",
    "    img_folder, lbl_folder = os.path.join(path, \"images\"), os.path.join(path, \"labels\")\n",
    "    if copy:\n",
    "        Utils.copy_files_lst(images, img_folder)\n",
    "        Utils.copy_files_lst(labels, lbl_folder)\n",
    "    Utils.check_txt_file_vs_images(os.path.join(path, \"images.txt\"), img_folder)\n",
    "\n",
    "for b in [8]:\n",
    "    copy_transect_batch_image_labels(collect_id=\"20240618_001_Iver3069_ABS2\", batch = b, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6cff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "# To zip a folder named 'my_files' located at 'C:\\Users\\data\\':\n",
    "# zip_folder(\"C:\\\\Users\\\\data\\\\my_files\") \n",
    "# Output: C:\\Users\\data\\my_files.zip (containing the my_files folder)\n",
    "\n",
    "# Example Usage for the original scenario (hypothetical):\n",
    "# batch_path = \"Z:\\\\...\\\\Goby\\\\transects\\\\AUV_Run_01\\\\AUV_Run_01_batch_01\"\n",
    "# zip_folder(os.path.join(batch_path, \"images\"), \"AUV_01_batch_01_images\")\n",
    "# zip_folder(os.path.join(batch_path, \"labels\"), \"AUV_01_batch_01_labels\")\n",
    "\n",
    "\n",
    "# --- Path Construction ---\n",
    "def get_im_lb_folders(batch, collect_id):\n",
    "    transect_update = f\"Z:\\\\__AdvancedTechnologyBackup\\\\04_ProjectData\\\\Innodata_2025\\\\Goby\\\\transects\\\\{collect_id}\"\n",
    "    batch_name = f\"{collect_id}_batch_{batch:02d}\"\n",
    "    path = os.path.join(transect_update, batch_name)\n",
    "    img_folder = os.path.join(path, \"images\")\n",
    "    lbl_folder = os.path.join(path, \"labels\")\n",
    "    return img_folder, lbl_folder\n",
    "\n",
    "\n",
    "# batches = [8]\n",
    "# collect_id = \"20240618_001_Iver3069_ABS2\"\n",
    "# for batch in batches:\n",
    "#     img_folder, lbl_folder = get_im_lb_folders(batch, collect_id)\n",
    "#     Utils.zip_folder(img_folder)\n",
    "#     Utils.zip_folder(lbl_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_path_0n = r\"..\\output\\validation\\detect\\test_sept_1fish\\test_run13_1n_curves.csv\"\n",
    "support_0n = df_abs2_meta_nf[(df_abs2_meta_nf.n_fish < 2)].shape[0]\n",
    "curve_path_2n = r\"..\\output\\validation\\detect\\test_sept_2-3fish\\test_run13_2-3n_curves.csv\"\n",
    "support_2n = df_abs2_meta_nf[(df_abs2_meta_nf.n_fish >= 2) & (df_abs2_meta_nf.n_fish <= 3)].shape[0]\n",
    "curve_path_4n = r\"..\\output\\validation\\detect\\test_sept_4fish+\\test_run13_4n+_curves.csv\"\n",
    "support_4n = df_abs2_meta_nf[(df_abs2_meta_nf.n_fish >= 4)].shape[0]\n",
    "curve_path_abs2 = r\"..\\output\\validation\\detect\\test_sept_abiss2\\test_run13_abiss2_curves.csv\"\n",
    "support_abs2 = df_abs2_meta_nf.shape[0]\n",
    "curve_path_abs1 = r\"..\\output\\validation\\detect\\test_sept_abiss1\\test_run13_abiss1_curves.csv\"\n",
    "support_abs1 = df_abs1_meta_nf.shape[0]\n",
    "df_0n, fmax_0n, cmax_0n, c_eq_0n, pr_eq_0n = Reports.get_metrics(curve_path_0n) \n",
    "df_2n, fmax_2n, cmax_2n, c_eq_2n, pr_eq_2n = Reports.get_metrics(curve_path_2n) \n",
    "df_4n, fmax_4n, cmax_4n, c_eq_4n, pr_eq_4n = Reports.get_metrics(curve_path_4n) \n",
    "dfabs2, fmaxabs2, cmaxabs2, c_eqabs2, pr_eqabs2 = Reports.get_metrics(curve_path_abs2) \n",
    "dfabs1, fmaxabs1, cmaxabs1, c_eqabs1, pr_eqabs1 = Reports.get_metrics(curve_path_abs1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_0n.precision, df_0n.recall, label=f\"ABISS2 only 1 fish, {support_0n}, {support_0n/support_abs2*100:0.1f}%\", color=\"purple\")\n",
    "plt.plot(df_2n.precision, df_2n.recall, label=f\"ABISS2 At 2-3 fish, {support_2n}, {support_2n/support_abs2*100:0.1f}%\", color=\"green\")\n",
    "plt.plot(df_4n.precision, df_4n.recall, label=f\"ABISS2 At least 4 fish, {support_4n}, {support_4n/support_abs2*100:0.1f}%\", color=\"orange\")\n",
    "plt.plot(dfabs1.precision, dfabs1.recall, label=f\"ABISS1 Test images, {support_abs1}\", color=\"red\")\n",
    "plt.xlabel('Recall') \n",
    "plt.ylabel('Precision') \n",
    "plt.title('Goby count bins Precision-Recall Curve')\n",
    "plt.legend() # 1461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8decc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dfabs2.precision, dfabs2.recall, label=f\"ABISS2 Test images, {support_abs2}\")\n",
    "plt.plot(dfabs1.precision, dfabs1.recall, label=f\"ABISS1 Test images, {support_abs1}\", color=\"red\")\n",
    "plt.xlabel('Recall') \n",
    "plt.ylabel('Precision') \n",
    "plt.title('ABISS1 vs ABISS2 Precision-Recall Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob(r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\\2025_goby_relabel_initial_sample\\full\\Innodata Output\\Initial Re-Label Full\\json\\*.json\")\n",
    "image_lst = glob.glob(r\"Z:\\__AdvancedTechnologyBackup\\04_ProjectData\\Innodata_2025\\Goby\\relabel\\2025_goby_relabel_initial_sample\\full\\original\\images\\images\\*.png\")\n",
    "image_path = image_lst[2]\n",
    "json_path = json_files[2]\n",
    "Overlays.plot_coco_boxes(image_path, json_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolov8v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
