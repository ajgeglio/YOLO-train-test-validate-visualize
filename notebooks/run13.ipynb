{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccebc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"src\"))\n",
    "from tqdm import tqdm\n",
    "from utils import Utils\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a783df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dictionary to map file paths for cleaner code\n",
    "PATHS = {\n",
    "    \"op_table\": r\"Z:\\__AdvancedTechnologyBackup\\07_Database\\OP_TABLE.xlsx\",\n",
    "    \"metadata\": r\"Z:\\__AdvancedTechnologyBackup\\07_Database\\MetadataCombined\\all_annotated_meta_splits_20250915.csv\",\n",
    "    \"output_dir\": r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\",\n",
    "    \"local_dir\": r\"D:\\datasets\",\n",
    "    \"addl_species\": r\"z:\\__AdvancedTechnologyBackup\\07_Database\\addl_species_log.xlsx\"\n",
    "}\n",
    "\n",
    "# Define the sets of collect_ids for each split\n",
    "COLLECT_IDS = {\n",
    "    \"transects\": {\"20200806_001_Iver3069_ABS1\", \"20200816_001_Iver3069_ABS1\",\n",
    "                  \"20210825_001_Iver3069_ABS1\", \"20210720_001_Iver3069_ABS1\"},\n",
    "    \"test\": {\"20200809_001_Iver3069_ABS1\", \"20200818_001_Iver3069_ABS1\", \"20200902_001_Iver3069_ABS1\", \"20200820_001_Iver3069_ABS1\", \"20200821_001_Iver3069_ABS1\", \"20200823_001_Iver3069_ABS1\",\n",
    "             \"20210811_001_Iver3069_ABS1\", \"20210812_001_Iver3069_ABS1\", \"20210812_002_Iver3069_ABS1\", \"20210719_001_Iver3069_ABS1\", \"20210829_001_Iver3069_ABS1\", \"20210911_001_Iver3069_ABS1\", \"20210911_002_Iver3069_ABS1\", \"20210925_001_Iver3069_ABS1\",\n",
    "             \"20220624_001_Iver3069_ABS1\", \"20220714_002_Iver3069_ABS1\", \"20220727_001_Iver3069_ABS2\", \"20220811_002_Iver3098_ABS2\", \"20220807_003_Iver3069_ABS2\", \"20220901_001_Iver3069_ABS2\", \"20220814_001_Iver3069_ABS2\", \"20220814_002_Iver3069_ABS2\",\n",
    "             \"20230710_001_Iver3098_ABS2\", \"20230909_001_Iver3069_ABS2\", \"20230810_002_Iver3098_ABS2\", \"20230727_001_Iver3098_ABS2\"},\n",
    "    \"validation\": {\"20200916_001_Iver3069_ABS1\", \"20200922_002_Iver3069_ABS1\", \"20200923_002_Iver3069_ABS1\",\n",
    "                   \"20210712_001_Iver3069_ABS1\", \"20210909_001_Iver3069_ABS1\", \"20210920_001_Iver3069_ABS1\", \"20210707_001_Iver3069_ABS1\", \"20210912_001_Iver3069_ABS1\", \"20210912_002_Iver3069_ABS1\", \"20210913_001_Iver3069_ABS1\",\n",
    "                   \"20220711_002_Iver3069_ABS1\", \"20220714_003_Iver3069_ABS1\", \"20220717_001_Iver3098_ABS2\", \"20220825_001_Iver3098_ABS2\", \"20220914_002_Iver3069_ABS2\", \"20220902_001_Iver3069_ABS2\",\n",
    "                   \"20230802_001_Iver3098_ABS2\", \"20230625_001_Iver3098_ABS2\", \"20230718_002_Iver3098_ABS2\", \"20230811_001_Iver3098_ABS2\", \"20230715_001_Iver3098_ABS2\"}\n",
    "}\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(PATHS[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# --- 1. Load and classify all metadata in one pass ---\n",
    "print(\"Loading and preparing metadata...\")\n",
    "all_annotated_meta = pd.read_csv(PATHS[\"metadata\"], low_memory=False)\n",
    "\n",
    "def get_split(collect_id):\n",
    "    for split_name, ids in COLLECT_IDS.items():\n",
    "        if collect_id in ids:\n",
    "            return split_name\n",
    "    return 'train'\n",
    "\n",
    "# Classify each row with its split\n",
    "all_annotated_meta['split'] = all_annotated_meta['collect_id'].apply(get_split)\n",
    "\n",
    "# Apply initial filters and exclude NaNs in 'DistanceToBottom_m' and any additional species\n",
    "addl_species_Filenames = pd.read_excel(PATHS[\"addl_species\"]).Filename\n",
    "\n",
    "filtered_meta = all_annotated_meta.query(\n",
    "    'Usability == \"Usable\" and ((year == 2021 and n_fish >= 2) or (year in [2020, 2022, 2023])) and not DistanceToBottom_m.isnull()'\n",
    ")\n",
    "filtered_meta = filtered_meta[~filtered_meta['Filename'].isin(addl_species_Filenames)]\n",
    "print(\"Filtered Usable data\", filtered_meta.shape)\n",
    "filtered_meta.to_csv(r\"Z:\\__AdvancedTechnologyBackup\\07_Database\\MetadataCombined\\all_annotated_meta_splits_filtered_20251030.csv\")\n",
    "# --- 2. Generate and save the text files for each split ---\n",
    "for split_name in ['train', 'test', 'validation', 'transects']:\n",
    "    # Get filenames for the current split\n",
    "    filenames = filtered_meta[filtered_meta['split'] == split_name]['Filename'].tolist()\n",
    "    \n",
    "    # Write each filename on a new line\n",
    "    output_path = os.path.join(PATHS[\"output_dir\"], f\"{split_name}.txt\")\n",
    "    output_path = os.path.join(PATHS[\"local_dir\"], f\"{split_name}.txt\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        # The key change is here: use '\\n'.join()\n",
    "        f.write('\\n'.join(filenames))\n",
    "        \n",
    "    print(f\"Saved {len(filenames)} filenames to {output_path}\")\n",
    "\n",
    "print(\"\\n--- Generating statistics reports ---\")\n",
    "\n",
    "# --- 3. Group and calculate summary statistics ---\n",
    "df_grouped = filtered_meta.groupby([\"collect_id\", \"split\"]).agg(\n",
    "    n_images=('Filename', 'count'),\n",
    "    n_fish_p_collect=('n_fish', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "df_grouped['n_fish_p_image'] = df_grouped['n_fish_p_collect'] / df_grouped['n_images']\n",
    "df_grouped['year'] = df_grouped['collect_id'].str[:4].astype(int)\n",
    "df_grouped['camera'] = df_grouped['collect_id'].str[-4:]\n",
    "\n",
    "# --- 4. Merge with op_table for additional info ---\n",
    "op_table = pd.read_excel(PATHS[\"op_table\"])\n",
    "df_stats = df_grouped.merge(\n",
    "    op_table[[\"COLLECT_ID\", \"LAKE_NAME\", \"MISSION_NAME\", \"PORT_NAME\", \"LATITUDE\", \"LONGITUDE\"]],\n",
    "    left_on=\"collect_id\",\n",
    "    right_on=\"COLLECT_ID\",\n",
    "    how='left'\n",
    ").drop(columns='COLLECT_ID')\n",
    "\n",
    "df_stats.to_csv(f\"{PATHS['output_dir']}/Run13_collect_stats.csv\", index=False)\n",
    "df_stats.to_csv(f\"{PATHS['local_dir']}/Run13_collect_stats.csv\", index=False)\n",
    "# --- 5. Export individual and combined summary CSVs ---\n",
    "for split_name, df_split in df_stats.groupby('split'):\n",
    "    print(f\"Total images for {split_name}:\", df_split['n_images'].sum())\n",
    "\n",
    "# combined_df.to_csv(f\"{PATHS['output_dir']}/yearly_split_stats.csv\", index=False)\n",
    "print(\"\\nAll stats CSVs generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de5543",
   "metadata": {},
   "source": [
    "--- Generating statistics reports ---\n",
    "Total images for test: 4460\n",
    "Total images for train: 31324\n",
    "Total images for transects: 7388\n",
    "Total images for validation: 3741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef784197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# --- Initial Data Loading ---\n",
    "# Note: The original path uses a Windows-style path string, which is fine, \n",
    "# but using a raw string (r\"...\") can prevent potential issues with backslashes.\n",
    "summary = pd.read_csv(r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\Run13_collect_stats.csv\")\n",
    "\n",
    "# --- Initial Data Cleanup/Prep ---\n",
    "# Simplify the camera column extraction immediately\n",
    "summary['camera'] = summary['collect_id'].str.split('_').str[-1]\n",
    "\n",
    "# --- Core Refactoring: Aggregate and Pivot ---\n",
    "\n",
    "# 1. Group by year, camera, and split, then sum n_images\n",
    "# 2. Pivot the 'split' column to make 'train', 'test', 'validation' separate columns\n",
    "df_pivot = summary.pivot_table(\n",
    "    index=['year', 'camera'],\n",
    "    columns='split',\n",
    "    values='n_images',\n",
    "    aggfunc='sum'\n",
    ").reset_index()\n",
    "\n",
    "# 3. Rename columns for clarity (matching your original final names)\n",
    "df_pivot = df_pivot.rename(columns={\n",
    "    'train': 'n images train',\n",
    "    'test': 'n images test',\n",
    "    'validation': 'n images validation'\n",
    "})\n",
    "\n",
    "# 4. Calculate total images and percentages\n",
    "total_col = (\n",
    "    df_pivot['n images train'] + \n",
    "    df_pivot['n images test'] + \n",
    "    df_pivot['n images validation']\n",
    ")\n",
    "df_pivot['n_images_total'] = total_col.astype(int)\n",
    "df_pivot['test (%)'] = (df_pivot['n images test'] / total_col * 100).round(2)\n",
    "df_pivot['validation (%)'] = (df_pivot['n images validation'] / total_col * 100).round(2)\n",
    "\n",
    "# --- Final Calculations (Based on the original logic) ---\n",
    "\n",
    "# Calculate n_tiles (now a single operation using np.where)\n",
    "# The logic: n_images * 9 if 'ABS2' is in 'camera', else n_images * 6\n",
    "df_pivot['n_tiles'] = np.where(\n",
    "    df_pivot['camera'].str.contains('ABS2', na=False),\n",
    "    total_col * 9,\n",
    "    total_col * 6\n",
    ").astype(int)\n",
    "\n",
    "# 5. Drop the individual split columns and rename the total column\n",
    "summary_run13 = df_pivot.drop(\n",
    "    columns=['n images train', 'n images test', 'n images validation']\n",
    ")\n",
    "summary_run13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a021798",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n tiles\", summary_run13.n_tiles.sum())\n",
    "print(\"n tiles by camera\", summary_run13.groupby(by=\"camera\").n_tiles.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ca86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_im_col = df_stats.n_images\n",
    "df_stats['n_tiles'] = np.where(\n",
    "    df_stats['camera'].str.contains('ABS2', na=False),\n",
    "    n_im_col * 9,\n",
    "    n_im_col * 6\n",
    ").astype(int)\n",
    "train_tiles = df_stats[df_stats['split'] == 'train'].n_tiles.sum()\n",
    "print(f\"Total tiles in train set: {train_tiles}\")\n",
    "test_tiles = df_stats[df_stats['split'] == 'test'].n_tiles.sum()\n",
    "print(f\"Total tiles in test set: {test_tiles}\")\n",
    "validation_tiles = df_stats[df_stats['split'] == 'validation'].n_tiles.sum()\n",
    "print(f\"Total tiles in validation set: {validation_tiles}\")\n",
    "transect_tiles = df_stats[df_stats['split'] == 'transects'].n_tiles.sum()\n",
    "print(f\"Total tiles in transects: {transect_tiles}\")\n",
    "# Total tiles in train set: 220317\n",
    "# Total tiles in test set: 31266\n",
    "# Total tiles in validation set: 26217\n",
    "# Total tiles in transects: 44328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6951ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Flull images txt files\n",
    "def write_split_txt_full(split, write=True):\n",
    "    dataset_folder = \"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\full\"\n",
    "    imgs = Utils.read_list_txt(f\"Z:\\\\__Organized_Directories_InProgress\\\\GobyFinderDatasets\\\\AUV_datasets\\\\{split}.txt\")\n",
    "    all_image_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\images\\\\*.png\")\n",
    "    all_image_paths += glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\images\\\\*.jpg\")\n",
    "    all_label_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\labels\\\\*.txt\")\n",
    "    images, labels  = Utils.list_full_set(imgs, all_image_paths, all_label_paths)\n",
    "    if len(imgs) < len(all_image_paths):\n",
    "        print(\"there are more images locally than in the split\")\n",
    "    elif len(imgs) > len(all_image_paths):\n",
    "        print(\"There are more images in the split than copied locally\")\n",
    "    assert len(images) == len(labels), \"Mismatch between number of images and labels\"\n",
    "    print(f\"Number of full images for {split}: {len(images)}\")\n",
    "    if write:\n",
    "        Utils.write_list_txt(images, f\"{dataset_folder}\\\\{split}\\\\images.txt\")\n",
    "        Utils.write_list_txt(labels, f\"{dataset_folder}\\\\{split}\\\\labels.txt\")\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e665403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of full images for train: 31324\n",
    "# Number of full images for test: 4460\n",
    "# Number of full images for validation: 3741\n",
    "splits = [\"train\", \"test\", \"validation\"]  # Change this to 'train', 'test', or 'transects' as needed\n",
    "for split in splits:\n",
    "    write_split_txt_full(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614876cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert validation images to jpg for the dataloader\n",
    "def convert_val_to_hq_jpg(VAL_OUTPUT_DIR = r\"D:\\datasets\\tiled\\validation\\jpg\", move_pngs = False):\n",
    "\n",
    "    validation_images = glob.glob(VAL_OUTPUT_DIR+\"\\\\\"+\"*png\")\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(VAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Iterate and convert all files\n",
    "    for filename in tqdm(validation_images):\n",
    "        if filename.lower().endswith('.png'):\n",
    "            png_path = filename\n",
    "            jpg_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "            jpg_path = os.path.join(VAL_OUTPUT_DIR, jpg_filename)\n",
    "            Utils.convert_png_to_highest_quality_jpeg(png_path, jpg_path)\n",
    "    if move_pngs: \n",
    "        Utils.MOVE_files_lst(validation_images, r\"D:\\datasets\\tiled\\validation\\png\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a501233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write tiled txt file paths\n",
    "def write_split_txt_tiled(split, dataset_folder=None, write=True):\n",
    "    image_folder = f'D:\\\\datasets\\\\tiled\\\\{split}\\\\images'\n",
    "    label_folder = f'D:\\\\datasets\\\\tiled\\\\{split}\\\\labels'\n",
    "    images = glob.glob(image_folder+\"\\\\\"+\"*.png\")\n",
    "    images += glob.glob(image_folder+\"\\\\\"+\"*.jpg\")\n",
    "    labels = glob.glob(label_folder+\"\\\\\"+\"*.txt\")\n",
    "    n_images = len(images)\n",
    "    n_labels = len(labels)\n",
    "    # assert n_images == n_labels\n",
    "    print(n_images, split)\n",
    "    if write:\n",
    "        Utils.write_list_txt(images, f\"{dataset_folder}\\\\{split}\\\\images.txt\")\n",
    "        Utils.write_list_txt(labels, f\"{dataset_folder}\\\\{split}\\\\labels.txt\")\n",
    "    return images, labels\n",
    "# 220419 train\n",
    "# 31266 test\n",
    "# 26217 validation\n",
    "dataset_folder = \"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\"\n",
    "splits = [\"train\", \"test\", \"validation\"]\n",
    "for split in splits:\n",
    "    write_split_txt_tiled(split, dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tiled_meta_csv(split=\"train\"):\n",
    "    img_pths, lbls_pths = write_split_txt_tiled(split, write=False)\n",
    "    imgs_pths_df = pd.DataFrame(np.c_[img_pths, lbls_pths], columns=[\"image_path\", \"label_path\"])\n",
    "    imgs_pths_df['Tilename'] = imgs_pths_df.image_path.apply(lambda x: os.path.basename(x).split(\".\")[0])\n",
    "    imgs_pths_df['Filename'] = imgs_pths_df.image_path.apply(lambda x: Utils.convert_tile_img_pth_to_basename(x))\n",
    "    filtered_meta_tiles = filtered_meta.drop(columns=[\"image_path\", \"label_path\"])\n",
    "    filtered_meta_tiles = pd.merge(imgs_pths_df, filtered_meta_tiles, on=\"Filename\", how=\"inner\")\n",
    "    filtered_meta_tiles = filtered_meta_tiles.rename(columns={\"Filename\":\"basename\", \"Tilename\": \"Filename\"})\n",
    "    filtered_meta_tiles['imw'] = 1672\n",
    "    filtered_meta_tiles['imh'] = 1307\n",
    "    return filtered_meta_tiles\n",
    "    \n",
    "\n",
    "def return_full_meta_csv(split=\"train\"):\n",
    "    img_pths, lbls_pths = write_split_txt_full(split, write=False)\n",
    "    imgs_pths_df = pd.DataFrame(np.c_[img_pths, lbls_pths], columns=[\"image_path\", \"label_path\"])\n",
    "    imgs_pths_df['Filename'] = imgs_pths_df.image_path.apply(lambda x: os.path.basename(x).split(\".\")[0])\n",
    "    filtered_meta_full = filtered_meta.drop(columns=[\"image_path\", \"label_path\"])\n",
    "    filtered_meta_full = pd.merge(imgs_pths_df, filtered_meta_full, on=\"Filename\", how=\"inner\")\n",
    "    return filtered_meta_full\n",
    "    \n",
    "\n",
    "# filtered_meta_tiles = return_tiled_meta_csv(split=\"test\")\n",
    "# filtered_meta_tiles.to_csv(f\"Z:\\\\__AdvancedTechnologyBackup\\\\07_Database\\\\MetadataCombined\\\\Run13_tiles_metadata_{split}.csv\")\n",
    "# filtered_meta_full = return_full_meta_csv(split=\"test\")\n",
    "# filtered_meta_full.to_csv(f\"Z:\\\\__AdvancedTechnologyBackup\\\\07_Database\\\\MetadataCombined\\\\Run13_full_metadata_{split}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed07ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31266 test\n",
      "26217 validation\n",
      "220419 train\n"
     ]
    }
   ],
   "source": [
    "filtered_meta_test = return_tiled_meta_csv(split=\"test\")\n",
    "filtered_meta_validation = return_tiled_meta_csv(split=\"validation\")\n",
    "filtered_meta_train = return_tiled_meta_csv(split=\"train\")\n",
    "filtered_meta_all = pd.concat([filtered_meta_test, filtered_meta_validation, filtered_meta_train], ignore_index=True)\n",
    "filtered_meta_all_abs2 = filtered_meta_all[filtered_meta_all.collect_id.str.contains(\"ABS2\")]\n",
    "filter = filtered_meta_all_abs2\n",
    "Utils.write_list_txt(filter.image_path, f\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\test_sets\\\\ABISS2_tiles\\\\images.txt\")\n",
    "Utils.write_list_txt(filter.label_path, f\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\test_sets\\\\ABISS2_tiles\\\\labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6057250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_meta_all.to_csv(f\"Z:\\\\__AdvancedTechnologyBackup\\\\07_Database\\\\MetadataCombined\\\\Run13_tiles_metadata_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for porportions\n",
    "train = Utils.read_list_txt(\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\\\\train\\\\images.txt\")\n",
    "test = Utils.read_list_txt(\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\\\\test\\\\images.txt\")\n",
    "valid = Utils.read_list_txt(\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\\\\validation\\\\images.txt\")\n",
    "# Train sest\n",
    "print(\"Train\", len(train)/(len(train)+len(test)+len(valid))*100)\n",
    "# Test set\n",
    "print(\"Test\", len(test)/(len(train)+len(test)+len(valid))*100)\n",
    "# Validation set\n",
    "print(\"Validation\", len(valid)/(len(train)+len(test)+len(valid))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3421a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hard-Negative mining (HNM) of the training set (Test run of training data completed with 2048 weights) \n",
    "\n",
    "lbl_report = pd.read_csv(r\"D:\\ageglio-1\\gobyfinder_yolov8\\output\\test_runs\\Labeled data tiled 2048 HNM\\label_box_results.csv\")\n",
    "print(\"n total objects\", len(lbl_report.Filename.unique()))\n",
    "print(\"n total background tiles\", len(lbl_report[lbl_report.conf.isna()].Filename.unique()))\n",
    "positive_filenames = lbl_report[~lbl_report.conf.isna()].Filename.unique()\n",
    "print(\"n postitive tiles (with labels)\", len(positive_filenames))\n",
    "scores = pd.read_csv(r\"D:\\ageglio-1\\gobyfinder_yolov8\\output\\test_runs\\Labeled data tiled 2048 HNM\\scores.csv\", index_col=0)\n",
    "print(\"Total objects on background tiles\", len(scores[(scores.ground_truth_id.isna()) & (scores.conf>=0.1)]), \"conf>=0.1\")\n",
    "print(\"n postitive tiles (with labels)\", len(lbl_report[~lbl_report.conf.isna()].Filename.unique()))\n",
    "print(\"Total Background Tiles with FPs (conf 0.1)\", len(scores[(scores.ground_truth_id.isna()) & (scores.conf>=0.1)].Filename.unique()))\n",
    "medium_negatives = scores[(scores.ground_truth_id.isna()) & (scores.conf>=0.1) & (scores.conf<0.2)]\n",
    "print(\"'Medium-Only' Negative Tiles\", len(medium_negatives.Filename.unique()))\n",
    "hardest_negatives = scores[(scores.ground_truth_id.isna()) & (scores.conf>=0.2)]\n",
    "hardest_negative_filenames = hardest_negatives.Filename.unique()\n",
    "print(\"Hardest Negative Tiles (conf 0.2)\", len(hardest_negatives.Filename.unique()))\n",
    "# 3. Combine these two lists\n",
    "# (You can't use 'scores' because it's missing the 456 pure-FN tiles where the predictor missed the whole image)\n",
    "# We just need the final list of filenames to keep.\n",
    "\n",
    "# Convert arrays to lists so you can add them\n",
    "all_positive_filenames_list = list(positive_filenames)\n",
    "all_hardest_negative_filenames_list = list(hardest_negative_filenames)\n",
    "\n",
    "# Combine them. This is your final set of filenames.\n",
    "all_filenames_to_keep = all_positive_filenames_list + all_hardest_negative_filenames_list\n",
    "\n",
    "print(\"Total tiles to keep in training:\", len(all_filenames_to_keep))\n",
    "# This output should now be 133360\n",
    "\n",
    "'''\n",
    "n total objects 220419\n",
    "n total background tiles 112247\n",
    "n postitive tiles (with labels) 108172\n",
    "Total objects on background tiles 57633 conf>=0.1\n",
    "n postitive tiles (with labels) 108172\n",
    "Total Background Tiles with FPs (conf 0.1) 41102\n",
    "'Medium-Only' Negative Tiles 22689\n",
    "Hardest Negative Tiles (conf 0.2) 25188\n",
    "Total tiles to keep in training: 133360\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply HNM filter for train set\n",
    "\n",
    "split=\"train\"\n",
    "tile_dir = f\"D:\\\\datasets\\\\tiled\\\\train\\\\images\"\n",
    "lbl_dir = f\"D:\\\\datasets\\\\tiled\\\\train\\\\labels\" \n",
    "dataset_folder = \"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\"\n",
    "all_tiles_to_keep = [os.path.join(tile_dir,f+\".png\") for f in all_filenames_to_keep]\n",
    "all_labels_to_keep = [os.path.join(lbl_dir,f+\".txt\") for f in all_filenames_to_keep]\n",
    "assert len(all_tiles_to_keep) == len(all_labels_to_keep)\n",
    "print(len(all_tiles_to_keep)) # 133360\n",
    "Utils.write_list_txt(all_tiles_to_keep, f\"{dataset_folder}\\\\{split}\\\\images.txt\")\n",
    "Utils.write_list_txt(all_labels_to_keep, f\"{dataset_folder}\\\\{split}\\\\labels.txt\")\n",
    "sanity_check = Utils.read_list_txt(r\"D:\\ageglio-1\\gobyfinder_yolov8\\datasets\\AUV_datasets\\run13\\tiled\\train\\images.txt\")\n",
    "assert len(sanity_check) == 133360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11969374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
