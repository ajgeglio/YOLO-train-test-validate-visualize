{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ccebc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"src\"))\n",
    "from tqdm import tqdm\n",
    "from utils import Utils\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a783df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing metadata...\n",
      "Filtered Usable data (46913, 170)\n",
      "Saved 31324 filenames to D:\\datasets\\train.txt\n",
      "Saved 4460 filenames to D:\\datasets\\test.txt\n",
      "Saved 3741 filenames to D:\\datasets\\validation.txt\n",
      "Saved 7388 filenames to D:\\datasets\\transects.txt\n",
      "\n",
      "--- Generating statistics reports ---\n",
      "Total images for test: 4460\n",
      "Total images for train: 31324\n",
      "Total images for transects: 7388\n",
      "Total images for validation: 3741\n",
      "\n",
      "All stats CSVs generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use a dictionary to map file paths for cleaner code\n",
    "PATHS = {\n",
    "    \"op_table\": r\"Z:\\__AdvancedTechnologyBackup\\07_Database\\OP_TABLE.xlsx\",\n",
    "    \"metadata\": r\"Z:\\__AdvancedTechnologyBackup\\07_Database\\MetadataCombined\\all_annotated_meta_splits_20250915.csv\",\n",
    "    \"output_dir\": r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\",\n",
    "    \"local_dir\": r\"D:\\datasets\",\n",
    "    \"addl_species\": r\"z:\\__AdvancedTechnologyBackup\\07_Database\\addl_species_log.xlsx\"\n",
    "}\n",
    "\n",
    "# Define the sets of collect_ids for each split\n",
    "COLLECT_IDS = {\n",
    "    \"transects\": {\"20200806_001_Iver3069_ABS1\", \"20200816_001_Iver3069_ABS1\",\n",
    "                  \"20210825_001_Iver3069_ABS1\", \"20210720_001_Iver3069_ABS1\"},\n",
    "    \"test\": {\"20200809_001_Iver3069_ABS1\", \"20200818_001_Iver3069_ABS1\", \"20200902_001_Iver3069_ABS1\", \"20200820_001_Iver3069_ABS1\", \"20200821_001_Iver3069_ABS1\", \"20200823_001_Iver3069_ABS1\",\n",
    "             \"20210811_001_Iver3069_ABS1\", \"20210812_001_Iver3069_ABS1\", \"20210812_002_Iver3069_ABS1\", \"20210719_001_Iver3069_ABS1\", \"20210829_001_Iver3069_ABS1\", \"20210911_001_Iver3069_ABS1\", \"20210911_002_Iver3069_ABS1\", \"20210925_001_Iver3069_ABS1\",\n",
    "             \"20220624_001_Iver3069_ABS1\", \"20220714_002_Iver3069_ABS1\", \"20220727_001_Iver3069_ABS2\", \"20220811_002_Iver3098_ABS2\", \"20220807_003_Iver3069_ABS2\", \"20220901_001_Iver3069_ABS2\", \"20220814_001_Iver3069_ABS2\", \"20220814_002_Iver3069_ABS2\",\n",
    "             \"20230710_001_Iver3098_ABS2\", \"20230909_001_Iver3069_ABS2\", \"20230810_002_Iver3098_ABS2\", \"20230727_001_Iver3098_ABS2\"},\n",
    "    \"validation\": {\"20200916_001_Iver3069_ABS1\", \"20200922_002_Iver3069_ABS1\", \"20200923_002_Iver3069_ABS1\",\n",
    "                   \"20210712_001_Iver3069_ABS1\", \"20210909_001_Iver3069_ABS1\", \"20210920_001_Iver3069_ABS1\", \"20210707_001_Iver3069_ABS1\", \"20210912_001_Iver3069_ABS1\", \"20210912_002_Iver3069_ABS1\", \"20210913_001_Iver3069_ABS1\",\n",
    "                   \"20220711_002_Iver3069_ABS1\", \"20220714_003_Iver3069_ABS1\", \"20220717_001_Iver3098_ABS2\", \"20220825_001_Iver3098_ABS2\", \"20220914_002_Iver3069_ABS2\", \"20220902_001_Iver3069_ABS2\",\n",
    "                   \"20230802_001_Iver3098_ABS2\", \"20230625_001_Iver3098_ABS2\", \"20230718_002_Iver3098_ABS2\", \"20230811_001_Iver3098_ABS2\", \"20230715_001_Iver3098_ABS2\"}\n",
    "}\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(PATHS[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# --- 1. Load and classify all metadata in one pass ---\n",
    "print(\"Loading and preparing metadata...\")\n",
    "all_annotated_meta = pd.read_csv(PATHS[\"metadata\"], low_memory=False)\n",
    "\n",
    "def get_split(collect_id):\n",
    "    for split_name, ids in COLLECT_IDS.items():\n",
    "        if collect_id in ids:\n",
    "            return split_name\n",
    "    return 'train'\n",
    "\n",
    "# Classify each row with its split\n",
    "all_annotated_meta['split'] = all_annotated_meta['collect_id'].apply(get_split)\n",
    "\n",
    "# Apply initial filters and exclude NaNs in 'DistanceToBottom_m' and any additional species\n",
    "addl_species_Filenames = pd.read_excel(PATHS[\"addl_species\"]).Filename\n",
    "\n",
    "filtered_meta = all_annotated_meta.query(\n",
    "    'Usability == \"Usable\" and ((year == 2021 and n_fish >= 2) or (year in [2020, 2022, 2023])) and not DistanceToBottom_m.isnull()'\n",
    ")\n",
    "filtered_meta = filtered_meta[~filtered_meta['Filename'].isin(addl_species_Filenames)]\n",
    "print(\"Filtered Usable data\", filtered_meta.shape)\n",
    "filtered_meta.to_csv(r\"Z:\\__AdvancedTechnologyBackup\\07_Database\\MetadataCombined\\all_annotated_meta_splits_filtered_20251030.csv\")\n",
    "# --- 2. Generate and save the text files for each split ---\n",
    "for split_name in ['train', 'test', 'validation', 'transects']:\n",
    "    # Get filenames for the current split\n",
    "    filenames = filtered_meta[filtered_meta['split'] == split_name]['Filename'].tolist()\n",
    "    \n",
    "    # Write each filename on a new line\n",
    "    output_path = os.path.join(PATHS[\"output_dir\"], f\"{split_name}.txt\")\n",
    "    output_path = os.path.join(PATHS[\"local_dir\"], f\"{split_name}.txt\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        # The key change is here: use '\\n'.join()\n",
    "        f.write('\\n'.join(filenames))\n",
    "        \n",
    "    print(f\"Saved {len(filenames)} filenames to {output_path}\")\n",
    "\n",
    "print(\"\\n--- Generating statistics reports ---\")\n",
    "\n",
    "# --- 3. Group and calculate summary statistics ---\n",
    "df_grouped = filtered_meta.groupby([\"collect_id\", \"split\"]).agg(\n",
    "    n_images=('Filename', 'count'),\n",
    "    n_fish_p_collect=('n_fish', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "df_grouped['n_fish_p_image'] = df_grouped['n_fish_p_collect'] / df_grouped['n_images']\n",
    "df_grouped['year'] = df_grouped['collect_id'].str[:4].astype(int)\n",
    "df_grouped['camera'] = df_grouped['collect_id'].str[-4:]\n",
    "\n",
    "# --- 4. Merge with op_table for additional info ---\n",
    "op_table = pd.read_excel(PATHS[\"op_table\"])\n",
    "df_stats = df_grouped.merge(\n",
    "    op_table[[\"COLLECT_ID\", \"LAKE_NAME\", \"MISSION_NAME\", \"PORT_NAME\", \"LATITUDE\", \"LONGITUDE\"]],\n",
    "    left_on=\"collect_id\",\n",
    "    right_on=\"COLLECT_ID\",\n",
    "    how='left'\n",
    ").drop(columns='COLLECT_ID')\n",
    "\n",
    "df_stats.to_csv(f\"{PATHS['output_dir']}/Run13_collect_stats.csv\", index=False)\n",
    "df_stats.to_csv(f\"{PATHS['local_dir']}/Run13_collect_stats.csv\", index=False)\n",
    "# --- 5. Export individual and combined summary CSVs ---\n",
    "for split_name, df_split in df_stats.groupby('split'):\n",
    "    print(f\"Total images for {split_name}:\", df_split['n_images'].sum())\n",
    "\n",
    "# combined_df.to_csv(f\"{PATHS['output_dir']}/yearly_split_stats.csv\", index=False)\n",
    "print(\"\\nAll stats CSVs generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de5543",
   "metadata": {},
   "source": [
    "--- Generating statistics reports ---\n",
    "Total images for test: 4460\n",
    "Total images for train: 31324\n",
    "Total images for transects: 7388\n",
    "Total images for validation: 3741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef784197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>camera</th>\n",
       "      <th>transects</th>\n",
       "      <th>n_images_total</th>\n",
       "      <th>test (%)</th>\n",
       "      <th>validation (%)</th>\n",
       "      <th>n_tiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>ABS1</td>\n",
       "      <td>4570.0</td>\n",
       "      <td>10104</td>\n",
       "      <td>11.22</td>\n",
       "      <td>9.82</td>\n",
       "      <td>60624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>ABS1</td>\n",
       "      <td>2818.0</td>\n",
       "      <td>13705</td>\n",
       "      <td>11.63</td>\n",
       "      <td>8.90</td>\n",
       "      <td>82230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>ABS1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2166</td>\n",
       "      <td>10.62</td>\n",
       "      <td>12.56</td>\n",
       "      <td>12996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>ABS2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5120</td>\n",
       "      <td>12.68</td>\n",
       "      <td>7.85</td>\n",
       "      <td>46080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>ABS2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8430</td>\n",
       "      <td>10.12</td>\n",
       "      <td>10.14</td>\n",
       "      <td>75870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split  year camera  transects  n_images_total  test (%)  validation (%)  \\\n",
       "0      2020   ABS1     4570.0           10104     11.22            9.82   \n",
       "1      2021   ABS1     2818.0           13705     11.63            8.90   \n",
       "2      2022   ABS1        NaN            2166     10.62           12.56   \n",
       "3      2022   ABS2        NaN            5120     12.68            7.85   \n",
       "4      2023   ABS2        NaN            8430     10.12           10.14   \n",
       "\n",
       "split  n_tiles  \n",
       "0        60624  \n",
       "1        82230  \n",
       "2        12996  \n",
       "3        46080  \n",
       "4        75870  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# --- Initial Data Loading ---\n",
    "# Note: The original path uses a Windows-style path string, which is fine, \n",
    "# but using a raw string (r\"...\") can prevent potential issues with backslashes.\n",
    "summary = pd.read_csv(r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\Run13_collect_stats.csv\")\n",
    "\n",
    "# --- Initial Data Cleanup/Prep ---\n",
    "# Simplify the camera column extraction immediately\n",
    "summary['camera'] = summary['collect_id'].str.split('_').str[-1]\n",
    "\n",
    "# --- Core Refactoring: Aggregate and Pivot ---\n",
    "\n",
    "# 1. Group by year, camera, and split, then sum n_images\n",
    "# 2. Pivot the 'split' column to make 'train', 'test', 'validation' separate columns\n",
    "df_pivot = summary.pivot_table(\n",
    "    index=['year', 'camera'],\n",
    "    columns='split',\n",
    "    values='n_images',\n",
    "    aggfunc='sum'\n",
    ").reset_index()\n",
    "\n",
    "# 3. Rename columns for clarity (matching your original final names)\n",
    "df_pivot = df_pivot.rename(columns={\n",
    "    'train': 'n images train',\n",
    "    'test': 'n images test',\n",
    "    'validation': 'n images validation'\n",
    "})\n",
    "\n",
    "# 4. Calculate total images and percentages\n",
    "total_col = (\n",
    "    df_pivot['n images train'] + \n",
    "    df_pivot['n images test'] + \n",
    "    df_pivot['n images validation']\n",
    ")\n",
    "df_pivot['n_images_total'] = total_col.astype(int)\n",
    "df_pivot['test (%)'] = (df_pivot['n images test'] / total_col * 100).round(2)\n",
    "df_pivot['validation (%)'] = (df_pivot['n images validation'] / total_col * 100).round(2)\n",
    "\n",
    "# --- Final Calculations (Based on the original logic) ---\n",
    "\n",
    "# Calculate n_tiles (now a single operation using np.where)\n",
    "# The logic: n_images * 9 if 'ABS2' is in 'camera', else n_images * 6\n",
    "df_pivot['n_tiles'] = np.where(\n",
    "    df_pivot['camera'].str.contains('ABS2', na=False),\n",
    "    total_col * 9,\n",
    "    total_col * 6\n",
    ").astype(int)\n",
    "\n",
    "# 5. Drop the individual split columns and rename the total column\n",
    "summary_run13 = df_pivot.drop(\n",
    "    columns=['n images train', 'n images test', 'n images validation']\n",
    ")\n",
    "summary_run13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a021798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n tiles 277800\n",
      "n tiles by camera camera\n",
      "ABS1    155850\n",
      "ABS2    121950\n",
      "Name: n_tiles, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(\"n tiles\", summary_run13.n_tiles.sum())\n",
    "print(\"n tiles by camera\", summary_run13.groupby(by=\"camera\").n_tiles.sum())\n",
    "# summary_run13.groupby(by=\"camera\").n_images_total.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7ca86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tiles in train set: 220317\n",
      "Total tiles in test set: 31266\n",
      "Total tiles in validation set: 26217\n"
     ]
    }
   ],
   "source": [
    "n_im_col = df_stats.n_images\n",
    "df_stats['n_tiles'] = np.where(\n",
    "    df_stats['camera'].str.contains('ABS2', na=False),\n",
    "    n_im_col * 9,\n",
    "    n_im_col * 6\n",
    ").astype(int)\n",
    "train_tiles = df_stats[df_stats['split'] == 'train'].n_tiles.sum()\n",
    "print(f\"Total tiles in train set: {train_tiles}\")\n",
    "test_tiles = df_stats[df_stats['split'] == 'test'].n_tiles.sum()\n",
    "print(f\"Total tiles in test set: {test_tiles}\")\n",
    "validation_tiles = df_stats[df_stats['split'] == 'validation'].n_tiles.sum()\n",
    "print(f\"Total tiles in validation set: {validation_tiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6951ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write tiled txt file paths\n",
    "def write_split_txt_tiled(split, write=True):\n",
    "    dataset_folder = \"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\"\n",
    "    new_image_folder = f'D:\\\\datasets\\\\tiled\\\\{split}\\\\images'\n",
    "    new_label_folder = f'D:\\\\datasets\\\\tiled\\\\{split}\\\\labels'\n",
    "    images = glob.glob(new_image_folder+\"\\\\\"+\"*.png\")\n",
    "    images += glob.glob(new_image_folder+\"\\\\\"+\"*.jpg\")\n",
    "    labels = glob.glob(new_label_folder+\"\\\\\"+\"*.txt\")\n",
    "    n_images = len(images)\n",
    "    n_labels = len(labels)\n",
    "    # assert n_images == n_labels\n",
    "    print(n_images, split)\n",
    "    if write:\n",
    "        Utils.write_list_txt(images, f\"{dataset_folder}\\\\{split}\\\\images.txt\")\n",
    "        Utils.write_list_txt(labels, f\"{dataset_folder}\\\\{split}\\\\labels.txt\")\n",
    "    return images, labels\n",
    "\n",
    "# Write Flull images txt files\n",
    "def write_split_txt_full(split, write=True):\n",
    "    dataset_folder = \"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\full\"\n",
    "    imgs = Utils.read_list_txt(f\"Z:\\\\__Organized_Directories_InProgress\\\\GobyFinderDatasets\\\\AUV_datasets\\\\{split}.txt\")\n",
    "    all_image_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\images\\\\*.png\")\n",
    "    all_image_paths += glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\images\\\\*.jpg\")\n",
    "    all_label_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\labels\\\\*.txt\")\n",
    "    images, labels  = Utils.list_full_set(imgs, all_image_paths, all_label_paths)\n",
    "    if len(imgs) < len(all_image_paths):\n",
    "        print(\"there are more images locally than in the split\")\n",
    "    elif len(imgs) > len(all_image_paths):\n",
    "        print(\"There are more images in the split than copied locally\")\n",
    "    assert len(images) == len(labels), \"Mismatch between number of images and labels\"\n",
    "    print(f\"Number of full images for {split}: {len(images)}\")\n",
    "    if write:\n",
    "        Utils.write_list_txt(images, f\"{dataset_folder}\\\\{split}\\\\images.txt\")\n",
    "        Utils.write_list_txt(labels, f\"{dataset_folder}\\\\{split}\\\\labels.txt\")\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e665403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of full images for train: 31324\n",
    "# Number of full images for test: 4460\n",
    "# Number of full images for validation: 3741\n",
    "# splits = [\"train\", \"test\", \"validation\"]  # Change this to 'train', 'test', or 'transects' as needed\n",
    "# for split in splits:\n",
    "#     write_split_txt_full(split)\n",
    "# 26217 validation\n",
    "# 220419 train\n",
    "# 31266 test\n",
    "# splits = [\"train\", \"test\", \"validation\"]\n",
    "# for split in splits:\n",
    "#     write_split_txt_tiled(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f40b247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26217 validation\n"
     ]
    }
   ],
   "source": [
    "def save_tiled_meta_csv(split=\"train\"):\n",
    "    train_imgs, train_lbls = write_split_txt_tiled(split, write=False)\n",
    "    train_imgs_df = pd.DataFrame(np.c_[train_imgs, train_lbls], columns=[\"image_path\", \"label_path\"])\n",
    "    train_imgs_df['Tilename'] = train_imgs_df.image_path.apply(lambda x: os.path.basename(x).split(\".\")[0])\n",
    "    train_imgs_df['Filename'] = train_imgs_df.image_path.apply(lambda x: Utils.convert_tile_img_pth_to_basename(x))\n",
    "    filtered_meta_tiles = filtered_meta.drop(columns=[\"image_path\", \"label_path\"])\n",
    "    filtered_meta_tiles = pd.merge(train_imgs_df, filtered_meta_tiles, on=\"Filename\", how=\"inner\")\n",
    "    filtered_meta_tiles = filtered_meta_tiles.rename(columns={\"Filename\":\"basename\", \"Tilename\": \"Filename\"})\n",
    "    filtered_meta_tiles['imw'] = 1672\n",
    "    filtered_meta_tiles['imh'] = 1307\n",
    "    filtered_meta_tiles.to_csv(f\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\output\\\\test_runs\\\\Labeled data tiled 2048 HNM\\\\tiles_metadata_{split}.csv\")\n",
    "save_tiled_meta_csv(split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ebe52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the tiles to train test split\n",
    "split = \"Train\"\n",
    "def move_all_tiled_imgs_to_splits(split):\n",
    "     dst_im_folder = r\"D:\\datasets\\full\\unusable\\tiled\\images\"\n",
    "     dst_lb_folder = r\"D:\\datasets\\full\\unusable\\tiled\\labels\"\n",
    "     # ... all your variable declarations ...\n",
    "     all_tiled_image_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\tiled\\\\images\\\\*.png\")\n",
    "     all_tiled_label_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\tiled\\\\labels\\\\*.txt\")\n",
    "     all_image_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\images\\\\*.png\")\n",
    "     all_label_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\labels\\\\*.txt\")\n",
    "     # Make sure all your destination directories exist\n",
    "     os.makedirs(dst_im_folder, exist_ok=True)\n",
    "     os.makedirs(dst_lb_folder, exist_ok=True)\n",
    "\n",
    "     imgs = Utils.read_list_txt(f\"Z:\\\\__Organized_Directories_InProgress\\\\GobyFinderDatasets\\\\AUV_datasets\\\\{split}.txt\")\n",
    "     b_images = set(imgs) # Use a set for faster lookups!\n",
    "\n",
    "     for im_path, lb_path in zip(all_tiled_image_paths, all_tiled_label_paths):\n",
    "          basename_tile_w_ext = os.path.basename(lb_path)\n",
    "          img_basename = basename_tile_w_ext.rsplit('_', 2)[0]\n",
    "          # Check if the image basename (e.g., \"001.png\") is NOT in the desired list\n",
    "          if img_basename not in b_images:\n",
    "               # Construct the label path using the image's basename (minus extension)\n",
    "               base_name_no_ext = os.path.splitext(basename_tile_w_ext)[0]\n",
    "               #    lb_path = os.path.join(label_dir, base_name_no_ext + \".txt\")\n",
    "\n",
    "               #    # Move files\n",
    "               shutil.move(im_path, dst_im_folder)\n",
    "               \n",
    "               #    # Add a check to ensure the corresponding label file actually exists before moving\n",
    "               if os.path.exists(lb_path):\n",
    "                    shutil.move(lb_path, dst_lb_folder)\n",
    "               else:\n",
    "                    print(f\"Warning: Label file not found for {basename_tile_w_ext} at {lb_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614876cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert validation images to jpg for the dataloader\n",
    "\n",
    "def convert_val_to_hq_jpg(VAL_OUTPUT_DIR = r\"D:\\datasets\\tiled\\validation\\jpg\", move_pngs = False):\n",
    "\n",
    "    validation_images = glob.glob(VAL_OUTPUT_DIR+\"\\\\\"+\"*png\")\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(VAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Iterate and convert all files\n",
    "    for filename in tqdm(validation_images):\n",
    "        if filename.lower().endswith('.png'):\n",
    "            png_path = filename\n",
    "            jpg_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "            jpg_path = os.path.join(VAL_OUTPUT_DIR, jpg_filename)\n",
    "            Utils.convert_png_to_highest_quality_jpeg(png_path, jpg_path)\n",
    "    if move_pngs: \n",
    "        Utils.MOVE_files_lst(validation_images, r\"D:\\datasets\\tiled\\validation\\png\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63b458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 79.31537016646156\n",
      "Test 11.250728674136926\n",
      "Validation 9.433901159401517\n"
     ]
    }
   ],
   "source": [
    "# final sanity check\n",
    "train = Utils.read_list_txt(\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\\\\train\\\\images.txt\")\n",
    "test = Utils.read_list_txt(\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\\\\test\\\\images.txt\")\n",
    "valid = Utils.read_list_txt(\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\\\\validation\\\\images.txt\")\n",
    "# Train sest\n",
    "print(\"Train\", len(train)/(len(train)+len(test)+len(valid))*100)\n",
    "# Test set\n",
    "print(\"Test\", len(test)/(len(train)+len(test)+len(valid))*100)\n",
    "# Validation set\n",
    "print(\"Validation\", len(valid)/(len(train)+len(test)+len(valid))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolov8v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
