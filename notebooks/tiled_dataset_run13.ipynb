{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccebc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"src\"))\n",
    "from tqdm import tqdm\n",
    "from utils import Utils\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a783df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing metadata...\n",
      "Filtered Usable data (46905, 170)\n",
      "Saved 31710 filenames to Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\train.txt\n",
      "Saved 4067 filenames to Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\test.txt\n",
      "Saved 3741 filenames to Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\validation.txt\n",
      "Saved 7387 filenames to Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\transects.txt\n",
      "\n",
      "--- Generating statistics reports ---\n",
      "Total images for test: 4067\n",
      "Total images for train: 31710\n",
      "Total images for transects: 7387\n",
      "Total images for validation: 3741\n",
      "\n",
      "All stats CSVs generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use a dictionary to map file paths for cleaner code\n",
    "PATHS = {\n",
    "    \"op_table\": r\"Z:\\__AdvancedTechnologyBackup\\01_DerivedProducts\\Database\\OP_TABLE.xlsx\",\n",
    "    \"meta_data\": r\"Z:\\__AdvancedTechnologyBackup\\01_DerivedProducts\\Database\\02__MetadataCombined\\all_annotated_meta_splits_20250915.csv\",\n",
    "    \"output_dir\": r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\",\n",
    "    \"addl_species\": r\"z:\\__AdvancedTechnologyBackup\\01_DerivedProducts\\Database\\addl_species_log.xlsx\"\n",
    "}\n",
    "\n",
    "# Define the sets of collect_ids for each split\n",
    "COLLECT_IDS = {\n",
    "    \"transects\": {\"20200806_001_Iver3069_ABS1\", \"20200816_001_Iver3069_ABS1\",\n",
    "                  \"20210825_001_Iver3069_ABS1\", \"20210720_001_Iver3069_ABS1\"},\n",
    "    \"test\": {\"20200809_001_Iver3069_ABS1\", \"20200818_001_Iver3069_ABS1\", \"20200902_001_Iver3069_ABS1\", \"20200820_001_Iver3069_ABS1\", \"20200821_001_Iver3069_ABS1\", \"20200823_001_Iver3069_ABS1\"\n",
    "             \"20210811_001_Iver3069_ABS1\", \"20210812_001_Iver3069_ABS1\", \"20210812_002_Iver3069_ABS1\", \"20210719_001_Iver3069_ABS1\", \"20210829_001_Iver3069_ABS1\", \"20210911_001_Iver3069_ABS1\", \"20210911_002_Iver3069_ABS1\", \"20210925_001_Iver3069_ABS1\",\n",
    "             \"20220624_001_Iver3069_ABS1\", \"20220714_002_Iver3069_ABS1\", \"20220727_001_Iver3069_ABS2\", \"20220811_002_Iver3098_ABS2\", \"20220807_003_Iver3069_ABS2\", \"20220901_001_Iver3069_ABS2\", \"20220814_001_Iver3069_ABS2\", \"20220814_002_Iver3069_ABS2\",\n",
    "             \"20230710_001_Iver3098_ABS2\", \"20230909_001_Iver3069_ABS2\", \"20230810_002_Iver3098_ABS2\", \"20230727_001_Iver3098_ABS2\"},\n",
    "    \"validation\": {\"20200916_001_Iver3069_ABS1\", \"20200922_002_Iver3069_ABS1\", \"20200923_002_Iver3069_ABS1\",\n",
    "                   \"20210712_001_Iver3069_ABS1\", \"20210909_001_Iver3069_ABS1\", \"20210920_001_Iver3069_ABS1\", \"20210707_001_Iver3069_ABS1\", \"20210912_001_Iver3069_ABS1\", \"20210912_002_Iver3069_ABS1\", \"20210913_001_Iver3069_ABS1\",\n",
    "                   \"20220711_002_Iver3069_ABS1\", \"20220714_003_Iver3069_ABS1\", \"20220717_001_Iver3098_ABS2\", \"20220825_001_Iver3098_ABS2\", \"20220914_002_Iver3069_ABS2\", \"20220902_001_Iver3069_ABS2\",\n",
    "                   \"20230802_001_Iver3098_ABS2\", \"20230625_001_Iver3098_ABS2\", \"20230718_002_Iver3098_ABS2\", \"20230811_001_Iver3098_ABS2\", \"20230715_001_Iver3098_ABS2\"}\n",
    "}\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(PATHS[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# --- 1. Load and classify all metadata in one pass ---\n",
    "print(\"Loading and preparing metadata...\")\n",
    "all_annotated_meta = pd.read_csv(PATHS[\"meta_data\"], low_memory=False)\n",
    "\n",
    "def get_split(collect_id):\n",
    "    for split_name, ids in COLLECT_IDS.items():\n",
    "        if collect_id in ids:\n",
    "            return split_name\n",
    "    return 'train'\n",
    "\n",
    "# Classify each row with its split\n",
    "all_annotated_meta['split'] = all_annotated_meta['collect_id'].apply(get_split)\n",
    "\n",
    "# Apply initial filters and exclude NaNs in 'DistanceToBottom_m' and any additional species\n",
    "addl_species_Filenames = pd.read_excel(PATHS[\"addl_species\"]).Filename\n",
    "\n",
    "filtered_meta = all_annotated_meta.query(\n",
    "    'Usability == \"Usable\" and ((year == 2021 and n_fish >= 2) or (year in [2020, 2022, 2023])) and not DistanceToBottom_m.isnull()'\n",
    ")\n",
    "filtered_meta = filtered_meta[~filtered_meta['Filename'].isin(addl_species_Filenames)]\n",
    "print(\"Filtered Usable data\", filtered_meta.shape)\n",
    "\n",
    "# --- 2. Generate and save the text files for each split ---\n",
    "for split_name in ['train', 'test', 'validation', 'transects']:\n",
    "    # Get filenames for the current split\n",
    "    filenames = filtered_meta[filtered_meta['split'] == split_name]['Filename'].tolist()\n",
    "    \n",
    "    # Write each filename on a new line\n",
    "    output_path = os.path.join(PATHS[\"output_dir\"], f\"{split_name}.txt\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        # The key change is here: use '\\n'.join()\n",
    "        f.write('\\n'.join(filenames))\n",
    "        \n",
    "    print(f\"Saved {len(filenames)} filenames to {output_path}\")\n",
    "\n",
    "print(\"\\n--- Generating statistics reports ---\")\n",
    "\n",
    "# --- 3. Group and calculate summary statistics ---\n",
    "df_grouped = filtered_meta.groupby([\"collect_id\", \"split\"]).agg(\n",
    "    n_images=('Filename', 'count'),\n",
    "    n_fish_p_collect=('n_fish', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "df_grouped['n_fish_p_image'] = df_grouped['n_fish_p_collect'] / df_grouped['n_images']\n",
    "df_grouped['year'] = df_grouped['collect_id'].str[:4].astype(int)\n",
    "df_grouped['camera'] = df_grouped['collect_id'].str[-4:]\n",
    "\n",
    "# --- 4. Merge with op_table for additional info ---\n",
    "op_table = pd.read_excel(PATHS[\"op_table\"])\n",
    "df_stats = df_grouped.merge(\n",
    "    op_table[[\"COLLECT_ID\", \"LAKE_NAME\", \"MISSION_NAME\", \"PORT_NAME\", \"LATITUDE\", \"LONGITUDE\"]],\n",
    "    left_on=\"collect_id\",\n",
    "    right_on=\"COLLECT_ID\",\n",
    "    how='left'\n",
    ").drop(columns='COLLECT_ID')\n",
    "\n",
    "df_stats.to_csv(f\"{PATHS['output_dir']}/Run13_collect_stats.csv\", index=False)\n",
    "# --- 5. Export individual and combined summary CSVs ---\n",
    "for split_name, df_split in df_stats.groupby('split'):\n",
    "    print(f\"Total images for {split_name}:\", df_split['n_images'].sum())\n",
    "\n",
    "# # Generate and export the combined yearly stats\n",
    "# combined_df = df_stats.groupby(['split', 'year']).agg(\n",
    "#     n_images=('n_images', 'sum')\n",
    "# ).reset_index()\n",
    "\n",
    "# combined_df.to_csv(f\"{PATHS['output_dir']}/yearly_split_stats.csv\", index=False)\n",
    "print(\"\\nAll stats CSVs generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de5543",
   "metadata": {},
   "source": [
    "--- Generating statistics reports ---\n",
    "Total images for test: 4067\n",
    "Total images for train: 31710\n",
    "Total images for transects: 7387\n",
    "Total images for validation: 3741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef784197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>camera</th>\n",
       "      <th>transects</th>\n",
       "      <th>n_images_total</th>\n",
       "      <th>test (%)</th>\n",
       "      <th>validation (%)</th>\n",
       "      <th>n_tiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>ABS1</td>\n",
       "      <td>4569.0</td>\n",
       "      <td>10098</td>\n",
       "      <td>10.71</td>\n",
       "      <td>9.82</td>\n",
       "      <td>60588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>ABS1</td>\n",
       "      <td>2818.0</td>\n",
       "      <td>13705</td>\n",
       "      <td>9.14</td>\n",
       "      <td>8.90</td>\n",
       "      <td>82230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>ABS1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2166</td>\n",
       "      <td>10.62</td>\n",
       "      <td>12.56</td>\n",
       "      <td>12996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>ABS2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5119</td>\n",
       "      <td>12.68</td>\n",
       "      <td>7.85</td>\n",
       "      <td>46071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>ABS2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8430</td>\n",
       "      <td>10.12</td>\n",
       "      <td>10.14</td>\n",
       "      <td>75870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split  year camera  transects  n_images_total  test (%)  validation (%)  \\\n",
       "0      2020   ABS1     4569.0           10098     10.71            9.82   \n",
       "1      2021   ABS1     2818.0           13705      9.14            8.90   \n",
       "2      2022   ABS1        NaN            2166     10.62           12.56   \n",
       "3      2022   ABS2        NaN            5119     12.68            7.85   \n",
       "4      2023   ABS2        NaN            8430     10.12           10.14   \n",
       "\n",
       "split  n_tiles  \n",
       "0        60588  \n",
       "1        82230  \n",
       "2        12996  \n",
       "3        46071  \n",
       "4        75870  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# --- Initial Data Loading ---\n",
    "# Note: The original path uses a Windows-style path string, which is fine, \n",
    "# but using a raw string (r\"...\") can prevent potential issues with backslashes.\n",
    "summary = pd.read_csv(r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\Run13_collect_stats.csv\")\n",
    "\n",
    "# --- Initial Data Cleanup/Prep ---\n",
    "# Simplify the camera column extraction immediately\n",
    "summary['camera'] = summary['collect_id'].str.split('_').str[-1]\n",
    "\n",
    "# --- Core Refactoring: Aggregate and Pivot ---\n",
    "\n",
    "# 1. Group by year, camera, and split, then sum n_images\n",
    "# 2. Pivot the 'split' column to make 'train', 'test', 'validation' separate columns\n",
    "df_pivot = summary.pivot_table(\n",
    "    index=['year', 'camera'],\n",
    "    columns='split',\n",
    "    values='n_images',\n",
    "    aggfunc='sum'\n",
    ").reset_index()\n",
    "\n",
    "# 3. Rename columns for clarity (matching your original final names)\n",
    "df_pivot = df_pivot.rename(columns={\n",
    "    'train': 'n images train',\n",
    "    'test': 'n images test',\n",
    "    'validation': 'n images validation'\n",
    "})\n",
    "\n",
    "# 4. Calculate total images and percentages\n",
    "total_col = (\n",
    "    df_pivot['n images train'] + \n",
    "    df_pivot['n images test'] + \n",
    "    df_pivot['n images validation']\n",
    ")\n",
    "df_pivot['n_images_total'] = total_col.astype(int)\n",
    "df_pivot['test (%)'] = (df_pivot['n images test'] / total_col * 100).round(2)\n",
    "df_pivot['validation (%)'] = (df_pivot['n images validation'] / total_col * 100).round(2)\n",
    "\n",
    "# --- Final Calculations (Based on the original logic) ---\n",
    "\n",
    "# Calculate n_tiles (now a single operation using np.where)\n",
    "# The logic: n_images * 9 if 'ABS2' is in 'camera', else n_images * 6\n",
    "df_pivot['n_tiles'] = np.where(\n",
    "    df_pivot['camera'].str.contains('ABS2', na=False),\n",
    "    total_col * 9,\n",
    "    total_col * 6\n",
    ").astype(int)\n",
    "\n",
    "# 5. Drop the individual split columns and rename the total column\n",
    "summary_run13 = df_pivot.drop(\n",
    "    columns=['n images train', 'n images test', 'n images validation']\n",
    ")\n",
    "summary_run13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a021798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277755"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_run13.n_tiles.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25b11c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322077"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_run13.n_tiles.sum() + (4569 + 2818)*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771e76b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "camera\n",
       "ABS1    155814\n",
       "ABS2    121941\n",
       "Name: n_tiles, dtype: int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary_run13.groupby(by=\"camera\").n_images_total.sum()\n",
    "summary_run13.groupby(by=\"camera\").n_tiles.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7ca86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tiles in train set: 222630\n",
      "Total tiles in test set: 28908\n",
      "Total tiles in validation set: 26217\n"
     ]
    }
   ],
   "source": [
    "n_im_col = df_stats.n_images\n",
    "df_stats['n_tiles'] = np.where(\n",
    "    df_stats['camera'].str.contains('ABS2', na=False),\n",
    "    n_im_col * 9,\n",
    "    n_im_col * 6\n",
    ").astype(int)\n",
    "train_tiles = df_stats[df_stats['split'] == 'train'].n_tiles.sum()\n",
    "print(f\"Total tiles in train set: {train_tiles}\")\n",
    "test_tiles = df_stats[df_stats['split'] == 'test'].n_tiles.sum()\n",
    "print(f\"Total tiles in test set: {test_tiles}\")\n",
    "validation_tiles = df_stats[df_stats['split'] == 'validation'].n_tiles.sum()\n",
    "print(f\"Total tiles in validation set: {validation_tiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380a5742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463683 463683\n"
     ]
    }
   ],
   "source": [
    "all_tiled_image_paths, all_tiled_label_paths = glob.glob(r\"D:\\datasets\\tiled\\images\\*.png\"), glob.glob(r\"D:\\datasets\\tiled\\labels\\*.txt\")\n",
    "print(len(all_tiled_image_paths), len(all_tiled_label_paths))\n",
    "assert len(all_tiled_image_paths) == len(all_tiled_label_paths), \"Mismatch between image and label counts.\"\n",
    "# 463683 463683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6951ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tiled images for validation: 26217\n"
     ]
    }
   ],
   "source": [
    "# Tiled set image lists\n",
    "\n",
    "split = \"validation\"  # Change this to 'train', 'test', or 'transects' as needed\n",
    "dataset_folder = \"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\tiled\"\n",
    "\n",
    "imgs = Utils.read_list_txt(f\"D:\\\\datasets\\\\{split}.txt\")\n",
    "tiled_images, tiled_labels  = Utils.list_tiled_set(imgs, all_tiled_image_paths, all_tiled_label_paths)\n",
    "assert len(tiled_images) == len(tiled_labels), \"Mismatch between number of images and labels\"\n",
    "print(f\"Number of tiled images for {split}: {len(tiled_images)}\")\n",
    "# Number of tiled images for train: 222732\n",
    "# Number of tiled images for test: 28908\n",
    "# Number of tiled images for validation: 26217\n",
    "# Utils.write_list_txt(tiled_images, f\"{dataset_folder}\\\\{split}\\\\images.txt\")\n",
    "# Utils.write_list_txt(tiled_labels, f\"{dataset_folder}\\\\{split}\\\\labels.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43dc5a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of full images for validation: 3741\n"
     ]
    }
   ],
   "source": [
    "# Flull images txt\n",
    "split = \"validation\"  # Change this to 'train', 'test', or 'transects' as needed\n",
    "dataset_folder = \"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\full\"\n",
    "\n",
    "imgs = Utils.read_list_txt(f\"D:\\\\datasets\\\\{split}.txt\")\n",
    "all_image_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\images\\\\*.jpg\")\n",
    "all_label_paths = glob.glob(f\"D:\\\\datasets\\\\full\\\\{split}\\\\labels\\\\*.txt\")\n",
    "images, labels  = Utils.list_full_set(imgs, all_image_paths, all_label_paths)\n",
    "assert len(images) == len(labels), \"Mismatch between number of images and labels\"\n",
    "print(f\"Number of full images for {split}: {len(images)}\")\n",
    "# Number of full images for validation: 3741\n",
    "# Number of full images for test: 4067\n",
    "# Number of full images for train: 31710\n",
    "Utils.write_list_txt(images, f\"{dataset_folder}\\\\{split}\\\\images.txt\")\n",
    "Utils.write_list_txt(labels, f\"{dataset_folder}\\\\{split}\\\\labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f4a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_images, corrupt_images = Utils.verify_images_corrected(images)\n",
    "# print(f\"Number of valid images: {len(valid_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "614876cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3706/3706 [00:00<00:00, 462842.15it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_images = images if split==\"validation\" else None\n",
    "# Example Usage:\n",
    "# Define your validation folder paths\n",
    "VAL_OUTPUT_DIR = r\"D:\\datasets\\full\\validation\\jpg\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(VAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Iterate and convert all files\n",
    "for filename in tqdm(validation_images):\n",
    "    if filename.lower().endswith('.png'):\n",
    "        png_path = filename\n",
    "        jpg_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        jpg_path = os.path.join(VAL_OUTPUT_DIR, jpg_filename)\n",
    "        \n",
    "        # Utils.convert_png_to_highest_quality_jpeg(png_path, jpg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c79b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 3706 / 3706  \r"
     ]
    }
   ],
   "source": [
    "# Utils.MOVE_files_lst(validation_images, r\"D:\\datasets\\full\\validation\\png\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd63b458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 80.31304612111542\n",
      "Test 10.30063571663754\n",
      "Validation 9.386318162247044\n"
     ]
    }
   ],
   "source": [
    "train = Utils.read_list_txt(\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\full\\\\train\\\\images.txt\")\n",
    "test = Utils.read_list_txt(\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\full\\\\test\\\\images.txt\")\n",
    "valid = Utils.read_list_txt(\"D:\\\\ageglio-1\\\\gobyfinder_yolov8\\\\datasets\\\\AUV_datasets\\\\run13\\\\full\\\\validation\\\\images.txt\")\n",
    "# Train sest\n",
    "print(\"Train\", len(train)/(len(train)+len(test)+len(valid))*100)\n",
    "# Test set\n",
    "print(\"Test\", len(test)/(len(train)+len(test)+len(valid))*100)\n",
    "# Validation set\n",
    "print(\"Validation\", len(valid)/(len(train)+len(test)+len(valid))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2a73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old Tile test\n",
    "# test_txt = r\"D:\\datasets\\images.txt\"\n",
    "# train_txt = r\"..\\datasets\\AUV_datasets\\run12\\train\\images.txt\"\n",
    "# validate_txt = r\"..\\datasets\\AUV_datasets\\run12\\validation\\images.txt\"\n",
    "# tiled_images_txt = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\tiled\\images.txt\"\n",
    "# tiled_labels_txt = r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\tiled\\labels.txt\"    \n",
    "# test_images = Utils.read_list_txt(test_txt)\n",
    "# tiled_images = Utils.read_list_txt(tiled_images_txt)\n",
    "# tiled_labels = Utils.read_list_txt(tiled_labels_txt)\n",
    "# tiled_test_images, tiled_test_labels = create_tiled_test_set(test_images, tiled_images, tiled_labels)\n",
    "# Utils.write_list_txt(tiled_test_images, r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\tiled\\test\\images.txt\")\n",
    "# Utils.write_list_txt(tiled_test_labels, r\"Z:\\__Organized_Directories_InProgress\\GobyFinderDatasets\\AUV_datasets\\tiled\\test\\labels.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolov8v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
